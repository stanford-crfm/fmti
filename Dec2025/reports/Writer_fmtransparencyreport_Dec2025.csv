Indicator,Definition,Notes,Disclosure,Justification,References,Score
Data acquisition methods,What methods does the developer use to acquire data used to build the model?,"Which of the following data acquisition methods does the developer use:  
(i) acquiring existing public datasets,
(ii) crawling the web,
(iii) using data acquired via its existing products and services,
(iv) licensing existing data from external parties,
(v) having humans create or annotate new data, 
(vi) using models to generate new data,  or
(vii) other data acquisition methods not captured by the above.

For example, if the developer uses reinforcement learning from human feedback to train models using model-generated outputs with human preference annotations, this would satisfy categories (v) and (vi).

Alternatively, if the developer post-trains its model using off-the-shelf preference data (for example, the Alpaca dataset), this would satisfy category (i).","Palmyra X5 has been trained on a mix of publicly available data from the Internet, data licensed from third parties, and using models to synthetically generate new data. We do not use customer data to train our model. Please see writer.com/trust. ","The developer acquires data from existing public datasets, crawling the web, licensing existing data from external parties, having humans create or annotate new data, and using models to generate new data. They do not use customer data to train their flagship model.",,1
Public datasets,What are the top-5 sources (by volume) of publicly available datasets acquired for building the model?,We define a source as the entity or means by which the developer acquires data. We define the top-5 sources as the top-5 sources by data volume. ,Not disclosed,No information provided.,,0
Crawling,"If data collection involves web-crawling, what is the crawler name and opt-out protocol?","We award this point for disclosure of the crawler name and opt-out protocols, including if/how they respect the Robots Exclusion Protocol (robots.txt).",Not disclosed,No information provided.,,0
Usage data used in training,What are the top-5 sources (by volume) of usage data from the developer's products and services that are used for building the model?,We define usage data as data collected from the use of a developer's products or services.,"Palmyra X5 has not been trained on any data provided to us by our users, which includes inputs and any output content. In other words, we do not use customer data to train our model. Please see writer.com/trust. ",No usage data used for training,,1
Notice of usage data used in training,"For the top-5 sources of usage data, how are users of these products and services made aware that this data is used for building the model?","We define usage data notice as the proactive disclosure to users of how their data is used for model development. For example, via a pop-up with a description, a link to the privacy policy, or link to a description of company practices. ","We do not use customer data (defined as data inputted into our Platform by our customers), including inputs and outputs, to train our models. We make this commitment clear in several places: (1) https://writer.com/trust/; and (2) in our Platform Services Agreement available at https://writer.com/legal/platform-services/. ",No usage data used for training,,1
Licensed data sources,What are the top-5 sources (by volume) of licensed data acquired for building the model?,"We define a source as the entity from which the developer acquires data. For example, the Associated Press is reportedly a source of licensed data for OpenAI.",Not disclosed,No information provided.,,0
Licensed data compensation,"For each of the top-5 sources of licensed data, are details related to compensation disclosed?",We award this point if the model developer describes the compensation structure specified in the contract with the data source or indicates they are prohibited from sharing this information if contractually mandated.,Not disclosed,No information provided.,,0
New human-generated data sources,What are the top-5 sources (by volume) of new human-generated data for building the model?,"We define a source as the entity or means by which the developer acquires data. For example, Scale AI could be a source of new human-generated data. By new, we mean the data is specifically acquired for the purposes of building the model.","We acquire new human-generated data from these sources: 

(1) Open-domain content aggregators: High-volume ingestion from filtered web crawls and other publicly available content to bootstrap general language comprehension.

(2) Enterprise data partnerships: Human-authored financial reports, healthcare records, and legal filings.","The specific sources of new human-generated data are not disclosed, and their relationship with licensed enterprise data and web crawled data is not clear.",,0
Instructions for data generation,"For each of the top-5 sources of human-generated data, what instructions does the developer provide for data generation?","The instructions should be those provided to the data source. For example, if a third-party vendor works directly with the data laborers to produce the data, the instructions from the developer to this vendor should be disclosed.","- Open-domain content aggregators: This data is filtered through automated systems designed to prioritize high-quality, well-formed, and safe language. No labor instructions were involved.
- Enterprise data partnerships: Datasets licensed from third parties must meet stringent legal, compliance, security and privacy (e.g. PII masking) requirements. They must also meet standards for quality, format, and structure.
- Annotation providers (e.g., upwork equivalents): Annotator instructions emphasized neutrality, accuracy, and task relevance. Within the finance domain, annotators were instructed to identify and label patterns of financial risk, extract key business metrics, and summarize complex earnings reports. Domain knowledge in financial accounting was a prerequisite. For data to be used in DPO, annotators used a structured evaluation rubric to rank model outputs based on helpfulness, factual accuracy, and user intent alignment. They received detailed onboarding and iterative feedback on early rounds to calibrate their judgments.
- Medical and legal subject matter experts (SMEs): Within the healthcare domain, documentation contributors were instructed to annotate medical records with appropriate codes, paraphrase physician notes in layperson-friendly language, and validate drug-diagnosis pairs. All work was done under strict privacy guidelines. Within the legal domain, reviewers were instructed to segment documents into functional categories (e.g., clauses, obligations), identify legally relevant language, and rewrite summaries.
- Synthetic instructional generation with human validation: Human evaluators were asked to assess data quality and relevance, validate labels, and audit for safety and bias.
","Annotators akin to Upwork crowdworkers were instructed to focus on neutrality, accuracy, and task relevance. For finance, annotators were instructed to identify and label patterns of financial risk, extract key business metrics, and summarize complex earnings reports. Domain knowledge in financial accounting was a prerequisite. For data to be used in DPO, annotators used a structured evaluation rubric to rank model outputs based on helpfulness, factual accuracy, and user intent alignment. They received detailed onboarding and iterative feedback on early rounds to calibrate their judgments. Within the healthcare domain, documentation contributors were instructed to annotate medical records with appropriate codes, paraphrase physician notes in layperson-friendly language, and validate drug-diagnosis pairs. All work was performed in line with our internal privacy guidelines and procedures. Within the legal domain, reviewers were instructed to segment documents into functional categories (e.g., clauses, obligations), identify legally relevant language, and rewrite summaries.",,1
Data laborer practices,"For the top-5 sources of human-generated data, how are laborers compensated, where are they located, and what labor protections are in place?","For each data source, we require (i) the compensation in either USD or the local currency, (ii) any countries where at least 25% of the laborers are located, and (iii) a description of any labor protections. We will award this point if the developer discloses that it is not aware of data laborer practices.","We are not aware of the data labor practices because to the extent we have acquired/used human-generated data by employees of third parties, we are not aware of their data labor practices.",The developer is not aware of the data labor practices and explains why that is the case.,,1
Synthetic data sources,What are the top-5 sources (by volume) of synthetic data acquired for building the model?,We define a source of synthetic data as a non-human mechanism (e.g. a machine learning model) used to generate the data.,We synthetically generate data using an internally developed model specifically designed to produce synthentic data.,"The developer confirms there is a single source of synthetic data, which is generated through an internal-only model.",,1
Synthetic data purpose,"For the top-5 sources of synthetically generated data, what is the primary purpose for data generation?",We define a source of synthetic data as a non-human mechanism (e.g. a machine learning model) used to generate the data.,"We use synthetic data generated from an internally developed model specifically designed for that purpose, the primary purpose of which is to create high quality, domain‑specific data to fine‑tune the model’s expertise for enterprise applications. ",Generating synthetic data for specific domains in relation to enterprise applications.,,1
Data processing methods,What are the methods the developer uses to process acquired data to determine the data directly used in building the model?,"We will award this point for disclosure of all of the methods used to process acquired data. Data processing refers to any method that substantively changes the content of the data. For example, compression or changing the data file format is generally not in the scope of this indicator.","Filtering: Language and quality filters were applied using a combination of regex-based heuristics and transformer-based toxicity classifiers. Enterprise-specific filters removed irrelevant documents from sensitive domains (e.g., consumer reviews from legal datasets).

Deduplication: Near-duplicate documents were identified using MinHash fingerprinting and cosine similarity over document embeddings. We retained one representative instance per cluster while prioritizing authoritative sources.

Normalization: We implemented domain-specific normalization functions (e.g., standardizing legal citations, converting dates to ISO format, unifying terminology across financial documents) to reduce variance without altering factual content.

Anonymization/De-identification: Protected health information (PHI), personally identifiable information (PII), and other identifiers were removed using hybrid rule-based and named entity recognition (NER) models. Anonymization scripts were evaluated for recall and precision by human reviewers.

Instruction Formatting: Alignment data was formatted into human preference pairs with detailed rubrics and quality assurance. Annotator instructions emphasized neutrality, accuracy, and task relevance.
","The methods for data processing (filtering, deduplication, normalization, anonymization, instruction formatting) are provided.",,1
Data processing purpose,"For each data processing method, what is its primary purpose?","Data processing refers to any method that substantively changes the content of the data. For example, compression or changing the data file format is generally not in the scope of this indicator.","Primary purposes include: removes low quality data, removes potentially personal data, removes toxic data, improves evaluation integrity, prepares the data for training the model.

",The purpose for each data processing method is provided.,,1
Data processing techniques,"For each data processing method, how does the developer implement the method?","Data processing refers to any method that substantively changes the content of the data. For example, compression or changing the data file format is generally not in the scope of this indicator.","Filtering: Language and quality filters were applied using a combination of regex-based heuristics and transformer-based toxicity classifiers. Enterprise-specific filters removed irrelevant documents from sensitive domains (e.g., consumer reviews from legal datasets).


Deduplication: Near-duplicate documents were identified using MinHash fingerprinting and cosine similarity over document embeddings. We retained one representative instance per cluster while prioritizing authoritative sources.


Normalization: We implemented domain-specific normalization functions (e.g., standardizing legal citations, converting dates to ISO format, unifying terminology across financial documents) to reduce variance without altering factual content.


Anonymization/De-identification: Protected health information (PHI), personally identifiable information (PII), other identifiers were removed using hybrid rule-based and named entity recognition (NER) models. Anonymization scripts were evaluated for recall and precision by human reviewers.


Instruction Formatting: Alignment data was formatted into human preference pairs with detailed rubrics and quality assurance. Annotator instructions emphasized neutrality, accuracy, and task relevance.

",The techniques for implementing each data processing method is provided.,,1
Data size,Is the size of the data used in building the model disclosed?,"To receive this point, the developer should report data size in appropriate units (e.g. bytes, words, tokens, images, frames) and broken down by modality. Data size should be reported to a precision of one significant figure (e.g. 4 trillion tokens, 200 thousand images). The size should reflect data directly used in building the model (i.e. training data) and not data that was acquired but unused, or data used to evaluate the model.","Text (Pretraining) - 4 trillion tokens
Text (Fine-Tuning) - 80 billion tokens
Alignment Data (Text) - 20 million pairs
Vision-Text Pairs - 200 million image-text pairs
OCR Documents - 15 million scanned docs

",4.08 trillion tokens; 20 million preference pairs; 200 million text-image pairs; 15 million OCR scanned documents,,1
Data language composition,"For all text data used in building the model, what is the composition of languages?","To receive this point, the developer should report (i) all languages which make up at least 1% of the data and their corresponding proportions and (ii) a brief description of how languages are labeled (if a publicly available tool is used, include a link to the tool). Proportions should be reported to a precision of two significant figures and should describe proportions of documents labeled with some langauge. An ""Unknown"" category may be included to denote documents where the language could not be identified.","
English - 82.00%
Spanish - 5.30%
French - 3.90%
German - 2.10%
Arabic - 1.70%
Unknown/Other - 5.00%

FastText’s language identification model (lid.176.bin) was used to label documents by language.

","Language distribution across English/Spanish/French/German/Arabic is provided, with 5% unknown/other, based on FastText language identification.",,1
Data domain composition,"For all the data used in building the model, what is the composition of domains covered in the data?","To receive this point, the developer should report the composition of the main domains included in the data used to train the model. This data should be at a level of granularity lower than broad claims about training on ""internet data"". For example, this could include the proportion of data from e-commerce, social media, news, code, etc. based on the URLs from which the data is sourced. Proportions should be reported to a precision of one significant figure.",Not disclosed,No information provided.,,0
External data access,Does a third-party have direct access to the data used to build the model?,"By a third-party, we mean entities that are financially independent of the developer.  We will award this point if at least one such entity is named as having direct access to the data. With that said, we may award this point if the developer provides justifications for prohibiting access to narrowly-scoped parts of the data. ","No third parties have access to the data used to build the model. We limit access (i.e. we do not provide external parties with access to training data) due to a mixture of concerns related to the proprietary nature of models we create, confidentiality, and security.",No external parties are provided access to training data though the developer explains why they do not disclose this information. ,,0
Data replicability,Is the data used to build the model described in enough detail to be externally replicable?,We will award this point if the description contains (i) a list of all publicly available training data and where to obtain it and (ii) a list of all training data obtainable from third parties and where to obtain it. These conditions refer to criteria 2 and 3 under the OSI Open Source AI v1.0 definition.,Not disclosed,No information provided.,,0
Compute usage for final training run,Is the amount of compute used in the model's final training run disclosed?,"Compute should be reported in appropriate units, which most often will be floating point operations (FLOPs), along with a description of the measurement methodology, which may involve estimation. Compute should be reported to a precision of one significant figure (e.g. 5 x 10^25 FLOPs). This number should represent the compute used to train the final model across all model stages.","1.2 x 10^24. FLOPs ≈ 6 × N_active × T, which reflects forward + backward (backward ≈ 2× forward) and the usual training update overhead. With N_active = 50B and T = 4T tokens, this is 6 × 5×10^10 × 4×10^12 ≈ 1.2 x 10^24 FLOPs, which we report (per guidance) as ~1 x 10^24 FLOPs to one significant figure. This matches common practice (the “6ND” rule) and detailed per-layer accounting differs by only ~1–10%.",1.2 x 10^24 FLOPs using conventional estimation methods.,,1
Compute usage including R&D,"Is the amount of compute used to build the model, including experiments, disclosed?","Compute should be reported in appropriate units, which most often will be floating point operations (FLOPs), along with a description of the measurement methodology, which may involve estimation. Compute should be reported to a precision of one significant figure (e.g. 7 x 10^26 FLOPs). Compared to the previous indicator, this indicator should include an estimation of the total compute used across experiments used towards the final training run for the model (such as including hyperparameter optimization or other experiments), and not just the final training run itself.","3 x 10^25 FLOPs. We start from the final-run estimate of ≈ 1.2 × 10^24 FLOPs using the standard 6 × N_active × T approximation for dense/decoder LLMs (forward + backward + update overhead). We then aggregate FLOPs across experiments that informed the final model—architecture/scaling ablations, hyperparameter sweeps on smaller/medium variants, data-mixture/tokenizer trials, MoE routing/top-k studies, alignment/SFT/RLHF iterations, and early-stopped runs—based on scheduler/job-accounting logs, normalizing each run via the same 6 N_active T method.",3 x 10^25 FLOPs with detailed discussion of estimation methodology,,1
Development duration for final training run,Is the amount of time required to build the model disclosed?,"The amount of time should be specified in terms of both the continuous duration of time required and the number of hardware hours used. The continuous duration of time required to build the model should be reported in weeks, days, or hours to a precision of one significant figure (e.g. 3 weeks). The number of hardware hours should be reported to a precision of one significant figure and include the type of hardware hours. No form of decomposition into phases of building the model is required for this indicator, but it should be clear what the duration refers to (e.g. training the model, or training and subsequent evaluation and red teaming).","Our model was trained over approximately 4 weeks of continuous training time – deployed in clusters of 512 GPUs (mixed A100/H100)

As such, estimated Total Compute Time = 340,000 GPU-hours (A100 and H100)

",4 weeks and 340000 Nvidia A100 and H100 GPU hours.,,1
Compute hardware for final training run,"For the primary hardware used to build the model, is the amount and type of hardware disclosed?","In most cases, this indicator will be satisfied by information regarding the number and type of GPUs or TPUs used to train the model. The number of hardware units should be reported to a precision of one significant figure (e.g. 800 NVIDIA H100 GPUs). We will not award this point if (i) the training hardware generally used by the developer is disclosed, but the specific hardware for the given model is not, or (ii) the training hardware is disclosed, but the amount of hardware is not. We will award this point even if information about the interconnects between hardware units is not disclosed.","Our model was trained using 512 NVIDIA H100s.

",512 Nvidia H100 GPUs,,1
Compute provider,Is the compute provider disclosed?,"For example, the compute provider may be the model developer in the case of a self-owned cluster, a cloud provider like Microsoft Azure, Google Cloud Platform, or Amazon Web Services, or a national supercomputer. In the event that compute is provided by multiple sources or is highly decentralized, we will award this point if a developer makes a reasonable effort to describe the distribution of hardware owners.","Compute is provided by Lambda Labs and AWS

","Lambda Labs, AWS",,1
Energy usage for final training run,Is the amount of energy expended in building the model disclosed?,"Energy usage should be reported in appropriate units, which most often will be megawatt-hours (mWh), along with a description of the measurement methodology, which may involve estimation. Energy usage should be reported to a precision of one significant figure (e.g. 500 mWh). No form of decomposition into compute phases is required, but it should be clear whether the reported energy usage is for a single model run or includes additional runs, or hyperparameter tuning, or training other models like reward models, or other steps in the model development process that necessitate energy usage. If the developer is unable to measure or estimate this quantity due to information not being available from another party (e.g. compute provider), we will award this point if the developer explicitly discloses what information it lacks and why it lacks it.","Estimated Total Compute Time: 340,000 GPU-hours (A100 and H100)

Average Power Consumption: ~0.4 kW per A100-equivalent GPU (including datacenter overhead)

Energy Usage Estimate:
 340,000 GPU-hours×0.4 kW=≈140 megawatt-hours (MWh)340{,}000 \text{ GPU-hours} \times 0.4 \text{ kW} = \boxed{≈ 140 \text{ megawatt-hours (MWh)}}340,000 GPU-hours×0.4 kW=≈140 megawatt-hours (MWh)​

Scope: Includes only the final model training (pretraining + fine-tuning). Does not include R&D or hyperparameter tuning.

",140 MWh with an estimation methodology of multiplying GPU time by average overhead-adjusted power consumption for GPU factor of 0.4,,1
Carbon emissions for final training run,Is the amount of carbon emitted in building the model disclosed?,"Emissions should be reported in appropriate units, which most often will be tons of carbon dioxide emitted (tCO2), along with a description of the measurement methodology, which may involve estimation. Emissions should be reported to a precision of one significant figure (e.g. 500 tCO2). No form of decomposition into compute phases is required, but it should be clear whether the reported emissions is for a single model run or includes additional runs, or hyperparameter tuning, or training other models like reward models, or other steps in the model development process that generate emissions. If the developer is unable to measure or estimate this quantity due to information not being available from another party (e.g. compute provider), we will award this point if the developer explicitly discloses what information it lack and why it lacks it. Emissions should correspond with the energy used in the previous indicator.","Methodology: We used the U.S. average grid carbon intensity of ~0.39 kg CO₂/kWh (EIA 2023).


Estimated Emissions:
 140,000 kWh×0.39 kg CO₂/kWh=≈55 metric tons of CO₂ (tCO₂)140{,}000 \text{ kWh} \times 0.39 \text{ kg CO₂/kWh} = \boxed{≈ 55 \text{ metric tons of CO₂ (tCO₂)}}140,000 kWh×0.39 kg CO₂/kWh=≈55 metric tons of CO₂ (tCO₂)​

Note: If cleaner energy sources were used (e.g., hydro or carbon-neutral credits), this figure could be lower, but such information was not available from the compute providers.

",55tCO2eq with estimation methodology of multiplying energy usage by average carbon intensity for US grid.,,1
Water usage for final training run,Is the amount of clean water used in building the model disclosed?,"Clean water usage should be in appropriate units, which most often will be megaliters, along with a description of the measurement methodology, which may involve estimation. Clean water usage should be reported to a precision of one significant figure (e.g., 5000ML). No form of decomposition into compute phases is required, but it should be clear whether the reported water usage is for a single model run or includes additional runs, or hyperparameter tuning, or training other models like reward models, or other steps in the model development process that necessitates water usage. If the developer is unable to measure or estimate this quantity due to information not being available from another party (e.g. compute provider), we will award this point if the developer explicitly discloses what information it lacks and why it lacks it.","Methodology: Water usage is estimated based on recent literature indicating ~0.2 liters of water used per kWh in datacenter cooling (adjusted for energy source and location).


Estimated Clean Water Usage:
 140,000 kWh×0.2 L/kWh=28,000 L=≈0.03 megaliters (ML)140{,}000 \text{ kWh} \times 0.2 \text{ L/kWh} = 28{,}000 \text{ L} = \boxed{≈ 0.03 \text{ megaliters (ML)}}140,000 kWh×0.2 L/kWh=28,000 L=≈0.03 megaliters (ML)​

Limitations: This figure is a lower-bound estimate. Actual water usage varies depending on local cooling systems (evaporative vs. air), regional climate, and energy grid mix.

","0.03ML with estimation methodology of multiplying energy usage by estimate of water usage for data center cooling, acknowledging it is a lower bound given other uses of water in data center operation.",,1
Internal compute allocation,How is compute allocated across the teams building and working to release the model?,"To receive a point, the developer should provide the compute allocated to each team involved in training the model. We understand there might be no clear allocation of compute across different teams; in that case, report an estimate of the compute used over the last year. Compute allocation should be reported to at least one significant figure.","Pretraining Infrastructure - 40%
Domain Fine-Tuning & RLHF - 25%
Research & Ablation Studies - 15%
Red-Teaming & Evaluation - 10%
Deployment Optimization -5%
Alignment & Safety Auditing - 5%

",Compute allocation across different components of model training is provided.,,1
Model stages,Are all stages in the model development process disclosed?,"Stages refer to each identifiable step that constitutes a substantive change to the model during the model building process. We recognize that different developers may use different terminology for these stages, or conceptualize the stages differently. We will award this point if there is a clear and complete description of these stages.","1. Pretraining on Diverse, High‑Quality Data (Stage 1 – Broad Corpus Training, Stage 2 – Domain‑Specific Refinement) 
2. Fine‑Tuning for Enterprise Applications
3. Direct Preference Optimization for Alignment

","3 stages (pretraining, which is split into 2 substages; fine-tuning; DPO) are described.",,1
Model objectives,"For all stages that are described, is there a clear description of the associated learning objectives or a clear characterization of the nature of this update to the model?","We recognize that different developers may use different terminology for these stages, or conceptualize the stages differently. We will award this point if there is a clear description of the update to the model related to each stage, whether that is the intent of the stage (e.g. making the model less harmful), a mechanistic characterization (e.g. minimizing a specific loss function), or an empirical assessment (e.g. evaluation results conducted before and after the stage).","The objective and purpose of the model training steps falls under the general umbrella of our training methodology. Put another way, these are the methods and thought processes we have that underly our model development, all of which are intended to promote reliability and accuracy and protect against the risks outlined  Pretraining 
We begin with pretraining on a high volume of diverse, high-quality data using a number of pre-processing techniques and analysis to promote data quality, format, filtering, and structure. We also employ several measures to safeguard individual privacy rights, including by obscuring and removing personal identifiers through a number of privacy enhancing techniques including differential privacy and k-anonymity. 

Fine-tuning
Following the pretraining phase, Palmyra X5 underwent supervised fine‑tuning across several key domains with a focus on enterprise use cases across finance, healthcare, legal, and other industry verticals. Our approach to targeted fine‑tuning makes Palmyra X5 not only powerful in general language understanding, but also highly adaptable to the specific needs of enterprise environments.

Direct preference optimization (DPO)
Finally, to align Palmyra X5 with user expectations and real‑world requirements, we utilize a direct preference optimization (DPO) approach. This strategy enhances stability in that it bypasses some of the instabilities found in traditional reinforcement learning from human feedback (RLHF), focuses on contrastive learning by leveraging contrastive losses on carefully selected preference pairs to improve the model’s output quality, and maintains efficiency in that it is particularly suited to long‑context learning tasks so that the model remains responsive even with extended input.

","While the developer describes an overall logic of high-level objectives for model training (i.e. maximizing accuracy, maximizing reliability, minimizing risks), the specific reason/objective for each step is not clearly articulated, with the disclosure instead addressing how the steps were operationalized procedurally (i.e. how the stage was executed rather than why).",,0
Code access,Does the developer release code that allows third-parties to train and run the model?,The released code does not need to match the code used internally. ,We do not release code underlying Palmyra X5,No code provided.,,0
Organization chart,How are employees developing and deploying the model organized internally? ,"To receive a point, the developer should provide both the internal organization chart for the team developing the model as well as the headcounts (or a proportion of headcounts) by the team.","Office of the CTO - 1

AI researchers (domain fine tuning & RLHF, research & ablation studies) - 8

ML Ops (Pretraining infra, deployment optimization) - 2

Content analyst (red teaming & evaluation, alignment & safety auditing) - 4

Partnerships - 1

Product marketing - 1

AI SWE - 5

",Headcount is provided across the organization.,,1
Model cost,What is the cost of building the model?,"Monetary cost should be reported in appropriate currency (e.g. USD), along with the measurement methodology, which may involve estimation. Cost should be reported to a precision of one significant figure (e.g. 200 million USD). ","Around 7-8million - with 6M on compute and around 1.5M around R&D

",$7.5M with 6M for compute and 1.5M for R&D,,1
Basic model properties,Are all basic model properties disclosed?,"Basic model properties include: the input modality, output modality, model size, model components, and model architecture. To receive a point, all model properties should be disclosed. Modalities refer to the types or formats of information that the model can accept as input. Examples of input modalities include text, image, audio, video, tables, graphs. Model components refer to distinct and identifiable parts of the model. We recognize that different developers may use different terminology for model components, or conceptualize components differently. Examples include: (i) For a text-to-image model, components could refer to a text encoder and an image encoder, which may have been trained separately. (ii) For a retrieval-augmented model, components could refer to a separate retriever module. Model size should be reported in appropriate units, which generally is the number of model parameters, broken down by named component. Model size should be reported to a precision of one significant figure (e.g. 500 billion parameters for text encoder, 20 billion parameters for image encoder). Model architecture is the overall structure and organization of a foundation model, which includes the way in which any disclosed components are integrated and how data moves through the model during training or inference. We recognize that different developers may use different terminology for model architecture, or conceptualize the architecture differently; a sufficient disclosure includes any clear, though potentially incomplete, description of the model architecture.","Input modality: Text, images
Output modality: Text, structured
Model components: Decoder-only model trained using self-supervised learning, followed by supervised fine-tuning and DPO that are used to align the model to follow users' instructions across several enterprise domains.
Model size: 456B
Model architecture: Hybrid transformer architecture that effectively balances dense and sparse computation using hybrid attention mechanisms and Mixture of Experts (MoE). This allows the model to maintain high levels of coherence across long contexts, without sacrificing efficiency or scale.

",All basic model properties are disclosed.,,1
Deeper model properties,Is a detailed description of the model architecture disclosed?,"To receive a point, the model architecture should be described in enough detail to allow for an external entity to fully implement the model. Publicly available code or a configuration file for a model training library (e.g., GPT-NeoX) would be a sufficiently detailed description.",Not disclosed,The developer does not disclose this information.,,0
Model dependencies,Is the model(s) the model is derived from disclosed?,"We will award this point for a comprehensive disclosure of the model or models on which the foundation model directly depends on or is derived from, as well as the method by which it was derived (e.g., through fine tuning, model merging, or distillation). Additionally, we will award a point if the developer discloses that the model is not dependent on or derived from any model.",Palmyra X5 is not dependent on or derived from any model.,The developer discloses that the model is not dependent on or derived from another model.,,1
Benchmarked inference,Is the compute and time required for model inference disclosed for a clearly-specified task on clearly-specified hardware?,The duration should be reported in seconds to a precision of one significant figure (e.g. 0.002 seconds). Compute usage for inference should be reported in FLOPs/second to a precision of one significant figure (e.g. 5 x 10^21 FLOPs/second). The hardware in this evaluation need not be the hardware the developer uses for inference. The developer can report this figure over some known or public dataset.,"We benchmarked Palmyra X5 on a long-context summarization task over a 2,048-token document input and a 256-token output length. Hardware is H100s. 

Around 0.12 seconds and 5 x 10^14 FLOPs/second

",Compute and time on specified hardware/tasks are disclosed.,,1
Researcher credits,Is a protocol for granting external entities API credits for the model disclosed?,"A model credit access protocol refers to the steps, requirements, and considerations involved in granting credits to external entities. We will award this point if the developer discloses key details of its protocol, including (i) where external entities can request access to credits (e.g. via an access request form); (ii) explicit criteria for selecting external entities; and (iii) its policy on granting a transparent decision on whether access has been granted within a specified, reasonable period of time. Additionally, we will award a point if the developer discloses that it does not grant external entities API credits.",We do not have a researcher access program. ,The developer discloses that they do not have a researcher access program.,,1
Specialized access,Does the developer disclose if it provides specialized access to the model?,"Specialized access could include several categories, such as early access, subsidized access, or deeper access (e.g., to model weights or checkpoints, that are not publicly available). We will award this point if the developer discloses (i) if it provides specialized access and (ii) statistics on the number of users granted access across academia, industry, non-profits, and governments, to one significant figure.","We provide early beta access to select customers. 

Update: We provided early access to a select group of approximately 10 existing customers with ~2 users per customer.","The statistics on specialized access granted (across academia, industry, non-profits, and governments to one significant figure) is not disclosed.",,0
Open weights,Are the model's weights openly released?,"To receive this point, model weights need to be publicly available at no cost. Developers may receive this point even if there are some restrictions on the external entities that are permitted access (e.g. geographic restrictions), insofar as these restrictions are transparent (e.g. via a license or some high-level description of who has been granted access to the foundation model).","Not applicable for Palmyra X5, which is a closed model",The weights are not openly released.,,0
Agent Protocols,Are the agent protocols supported for the model disclosed?,"Agent protocols are specifications that define how autonomous agents exchange messages, context, or function calls with other agents, tools, or services (e.g., Anthropic’s Model Context Protocol (MCP) and Google’s Agent‑to‑Agent (A2A) spec). To earn this point, documentation must enumerate each protocol and describe any deviations or proprietary extensions. ","We support MCP and A2A for agents built using Palmyra X5.

Please see https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/   ",The developer discloses the agent protocols supported (MCP and A2A).,,1
Capabilities taxonomy,Are the specific capabilities or tasks that were optimized for during post-training disclosed?,"Capabilities refer to the specific and distinctive functions that the model can perform. We recognize that different developers may use different terminology for capabilities, or conceptualize capabilities differently. We will award this point for a list of capabilities specifically optimized for in the post-training phase of the model, even if some of the capabilities are not reflected in the final model.","We focus on the following capabilities during post-training: 
- Domain-specific knowledge
- Tool use
- Retrieval

",The developer discloses the capabilities optimized for during post-training.,,1
Capabilities evaluation,Does the developer evaluate the model's capabilities prior to its release and disclose them concurrent with release?,"The evaluations must contain precise quantifications of the model's behavior in relation to the capabilities specified in the capabilities taxonomy. We will award this point for any clear, but potentially incomplete, evaluation of multiple capabilities.","We evaluate capabilities using the following benchmarks:

BBH 
GPQA
IFEval
MATH_HARD
MMLU-Pro
MuSR
OpenAI-MRCR
BigCodeBench



Updated: Please see https://writer.com/engineering/long-context-palmyra-x5/, which includes evaluation results.","The provided link includes evaluation results for all of the benchmarks. The specific evaluation results corresponding to at least two of the capabilities specified in the previous indicator can also be reasonably inferred: e.g., MMLU-Pro for Domain-specific knowledge and OpenAI-MRCR for retrieval.",,1
External reproducibility of capabilities evaluation,Are code and prompts that allow for an external reproduction of the evaluation of model capabilities disclosed?,"The released code and prompts need not be the same as what is used internally, but should allow the developer's results on all capability evaluations to be reproduced. The released code must be open source, following the OSI definition of open source.",We do not disclose code and prompts for an external reproduction of the evaluation of model capabilities. ,The developer does not disclose this information.,,0
Train-test overlap,Does the developer measure and disclose the overlap between the training set and the dataset used to evaluate model capabilities?,"We will award this point if, with every capability evaluation for which the developer reports results, the developer reports the overlap between the training set of the model and the dataset used for evaluation, as well as the general methodology for computing train-test overlap (e.g. a description of how n-gram matching was used). ","We evaluated train-test overlap using n-gram matching using the procedure described here: https://github.com/stanford-crfm/data-overlap 

","The methodology is disclosed, but the amount of overlap itself is not.",,0
Risks taxonomy,Are the risks considered when developing the model disclosed?,"Risks refer to possible negative consequences or undesirable outcomes that can arise from the model's deployment and usage. These consequences or outcomes may arise from model limitations (functions that the model cannot perform) or issues with the model's trustworthiness (e.g., its lack of robustness, reliability, calibration). We recognize that different developers may use different terminology for risks, or conceptualize risks differently. We will award this point for a complete list of risks considered, even if some of the risks are not reflected in the final model.","We evaulate a number of risks during the model development release and disclose those risks an technical report we will make available to our customers under NDA. These risks include, but are not necessarily limited to, general model safety and usage, accuracy, bias and discrimination, misinformation and harmful content, privacy, and security.



Updated: To clarify, the risks we evaluated included: (1) bias and discrimination; (2) misinformation and harmful content; (3) explainability/transparency - put another way, reliability and trustworthiness; and (4) privacy and security.",The developer discloses the risks considered when developing the model.,,1
Risks evaluation,Does the developer evaluate the model's risks prior to its release and disclose them concurrent with release?,The evaluations must contain precise quantifications of the model's behavior in relation to the risks specified in the risk taxonomy. We will award this point for clear evaluations of the majority of the states risks.,Not disclosed,The developer does not disclose this information.,,0
External reproducibility of risks evaluation,Are code and prompts to allow for an external reproduction of the evaluation of model risks disclosed?,"The released code and prompts need not be the same as what is used internally, but should allow the developer's results on all risk evaluations to be reproduced. The released code must be open-source, following the OSI definition of open-source.",We do not disclose code and prompts for an external reproduction of the evaluation of model capabilities. ,The developer does not disclose this information.,,0
Pre-deployment risk evaluation,Are the external entities have evaluated the model pre-deployment disclosed?,"By external entities, we mean entities that are significantly or fully independent of the developer. We will award this point if the developer specifies the entity that carried out the pre-deployment analysis, discloses the terms of the analysis (such as conditions for releasing the evaluation results or the developer's control over the final results), as well as any financial transaction between the parties. We will award this point if the developer discloses no external entities have evaluated the model pre-deployment, or discloses only terms of the analysis where it is not bound by NDA while still naming all external entities.","We provide the following parties access to our model for pre-deployment capabilities evaluation: BFCL. BFCL has control over the release of the evaluation results (including whether or not to release the results and the contents of the results being released), but must provide the evaluation results to us for review before release. There are no financial transactions between us and BFCL.

","The developer discloses the entities that carried out pre-deployment evaluations, information about the terms of the evaluation, and (the lack of) financial transactions between the parties.",,1
External risk evaluation,Are the parties contracted to evaluated model risks disclosed?,"We will award this point if the developer discloses statistics regarding all contracted parties that are responsible for evaluating risks (not limited to external entities or pre-deployment evaluation). This includes the number of contracted for-profit or non-profit entities, government entities, independent contractors, and researchers contracted by the developer to evaluate risks. We will award this point if the developer discloses it has no such contracts.",We have contracted with one outside party to perform bias and other benchmarking assessments designed to evaluate risks associated with Palmyra X5.,"The developer discloses that there is one contracted party, though the entity is not specified and it is unclear what risk evaluations the Berkeley Function Calling Leaderboard carries out.",,0
Mitigations taxonomy,Are the post-training mitigations implemented when developing the model disclosed?,"By post-training mitigations, we refer to interventions implemented by the developer during the post-training phase to reduce the likelihood and/or the severity of the model’s risks. We recognize that different developers may use different terminology for mitigations, or conceptualize mitigations differently. We will award this point for a complete list of mitigations considered, even if some of the mitigations are not reflected in the final model. Alternatively, we will award this point if the developer reports that it does not mitigate risk in this way.","Following the pretraining phase, Palmyra X5 underwent supervised fine‑tuning across several key domains with a focus on enterprise use cases across finance, healthcare, legal, and other industry verticals. Our approach to targeted fine‑tuning makes Palmyra X5 not only powerful in general language understanding, but also highly adaptable to the specific needs of enterprise environments.

Finally, to align Palmyra X5 with user expectations and real‑world requirements, we utilize a direct preference optimization (DPO) approach. This strategy enhances stability in that it bypasses some of the instabilities found in traditional reinforcement learning from human feedback (RLHF), focuses on contrastive learning by leveraging contrastive losses on carefully selected preference pairs to improve the model’s output quality, and maintains efficiency in that it is particularly suited to long‑context learning tasks so that the model remains responsive even with extended input.



Updated: To add more context, we do implement supervised fine tuning and reinforecement learning with human feedback to mitigate/address risks. ",The developer discloses that they implement SFT & RLHF  to mitigate risks.,,1
Mitigations taxonomy mapped to risk taxonomy,Does the developer disclose how the post-training mitigations map onto the taxonomy of risks?,"We will award this point for a complete mapping of the primary risk that each mitigation is meant to address, even if the mitigation potentially maps on to other risks in the taxonomy. Alternatively, we will award this point if the developer reports that it does not mitigate risk.","Following the pretraining phase, Palmyra X5 underwent supervised fine‑tuning across several key domains with a focus on enterprise use cases across finance, healthcare, legal, and other industry verticals. Our approach to targeted fine‑tuning makes Palmyra X5 not only powerful in general language understanding, but also highly adaptable to the specific needs of enterprise environments.

Finally, to align Palmyra X5 with user expectations and real‑world requirements, we utilize a direct preference optimization (DPO) approach. This strategy enhances stability in that it bypasses some of the instabilities found in traditional reinforcement learning from human feedback (RLHF), focuses on contrastive learning by leveraging contrastive losses on carefully selected preference pairs to improve the model’s output quality, and maintains efficiency in that it is particularly suited to long‑context learning tasks so that the model remains responsive even with extended input.



Updated: Please see our supplementary response to Question 53, which sets out our taxonomy of risks.",The developer does not produce a complete mapping from each mitigation (SFT and RLHF) to the taxonomy of risks.,,0
Mitigations efficacy,Does the developer evaluate and disclose the impact of post-training mitigations?,"We will award this point if the developer discloses the results on the risk evaluations before and after the post-training mitigations are applied. Alternatively, we will award this point if the developer reports that it does not mitigate risk in this way.",We do not mitigate risk in this way.,"The developer discloses that they ""do not mitigate risk in this way"", however in a previous indicator they specify that they use SFT & RLHF for risk mitigation.",,0
External reproducibility of mitigations evaluation,Are code and prompts to allow for an external reproduction of the evaluation of post-training mitigations disclosed?,"The released code and prompts need not be the same as what is used internally, but should allow the developer's results on all mitigations evaluations to be reproduced. The released code must be open-source, following the OSI definition of open-source. Alternatively, we will award this point if the developer reports that it does not mitigate risk.",We do not disclose code and prompts for an external reproduction of the evaluation of model capabilities. ,The developer does not disclose this information.,,0
Model theft prevention measures,Does the developer disclose the security measures used to prevent unauthorized copying (“theft”) or unauthorized public release of the model weights?,"This indicator assesses the developer's disclosures regarding how it addresses the risk that malicious actors or insiders could exfiltrate or replicate proprietary weights. Security measures could include insider threat analysis and detection, in addition to external threat management. Examples of such measures include encryption at rest, key management, remote attestation, or auditing for suspicious queries. We will award a point if the developer discloses specific steps taken to safeguard the model weights or that none are implemented.","Our model weights are securely stored in a private repository with enhanced security, and access is strictly controlled on a need-to-know basis. 

We continuously collaborate with security vendors to update and strengthen our security protocols, ensuring both dynamic and static protection against various forms of attacks and leaks. During inference, we implement robust security measures, including real-time monitoring and anomaly detection, to safeguard the model weights. 

Additionally, we encrypt the weights and conduct regular security audits and penetration testing to maintain the integrity and confidentiality of the data. We also have a well-defined incident response plan to quickly address any security breaches or leaks, taking immediate actions to contain and resolve issues, followed by a thorough post-incident analysis.

",The developer discloses the security measures used to mitigate model theft.,,1
Release stages,Are the stages of the model's release disclosed?,"Release stages include A/B testing, release on a user-facing product, GA release, open-weight release, etc.  We recognize that the release of a foundation model falls along a spectrum, with many forms of partial release, and that different developers may conceptualize release differently. We will award a point if the developer provides a clear identification of the stages through which the model was released.","1. Internal beta
2. Closed customer beta
3. GA",The developer outlines the stages of model release.,,1
Risk thresholds,Are risk thresholds disclosed?,"Risk thresholds determine when a risk level is unacceptably high to a developer (e.g. leading to the decision to not release a model), moderately high (e.g. triggering additional safety screening), or low enough to permit normal usage.

We will award this point if the developer discloses explicit risk thresholds that clarify (i) which harmful outcomes are being scored, (ii) how the scores are computed (in general terms, not necessarily disclosing internal algorithms), and (iii) what triggers an action to block, delay, or otherwise modify a model's release. 

Alternatively, we will award a point if the developer discloses that it does not consider explicit risk thresholds during model release. ","While Palmyra X5 was released following internal red teaming and SME evaluation, no formal numeric risk gating thresholds were published at the time of launch.

Trigger thresholds are disclosed as following: 

Toxicity: >3% flagged completions on eval prompts; hitting this threshold triggers red teaming + block

Hallucination: F1 score <75 on QA benchmarks; hitting this threshold triggers delaying release + retraining model

Bias disparity: >15% output skew across identity prompts; triggers alignment tuning

Privacy leakage: Any memorized PII in public benchmarks; blocks release + audit training

",The developer discloses risk thresholds.,,1
Versioning protocol,Is there a disclosed protocol for versioning and deprecation of the model?,"We will award a point if the developer discloses how model versions are labeled, updated, deprecated, and communicated to users.","We version models based on naming convention ie Palmyra X4, Palmyra X5. 

We’ll announce the deprecation of a model at least three months in advance. This will give customers time to plan for the migration to the new model.
We’ll continue to support deprecated models for a period of time after they’re deprecated. This will give customers time to migrate to the new model.
We’ll eventually stop supporting deprecated models. The timeline for this will vary depending on the model. We will announce the end of support for a deprecated model at least six months in advance. Please see https://dev.writer.com/home/models#timeline-for-deprecation. 
​
",The developer discloses a versioning protocol. The developer also discloses their deprecation/communication protocol.,,1
Change log,Is there a disclosed change log for the model?,"We will award a point if the developer publishes a version-by-version record of new features, fixes, or performance improvements.",This is a newly released model - we do not have new versions. ,"In their ""Developer Portal"" (https://dev.writer.com/home/changelog), the developer discloses change log that lists new features.",https://dev.writer.com/home/changelog,1
Foundation model roadmap,"Is a forward-looking roadmap for upcoming models, features, or products disclosed?","A foundation model roadmap is a transparent statement about how the developer intends to evolve or expand its LLM offerings, including upcoming models, major feature releases, or expanded products based on the model, along with approximate timelines or version milestones. It can be high-level (e.g., “new model Q2 2025”), but must exist publicly.","We share forward-looking roadmaps for upcoming models, features, and products with some customers under NDA.

Updated: We have publicly disclosed forward-looking product roadmaps, most recently with our launch of AI HQ (https://writer.com/blog/writer-ai-hq-press-release/). We also have various events throughout the year called AI Leaders Forums (e.g. https://www.linkedin.com/posts/getwriter_live-from-new-york-our-ai-leaders-forum-activity-7335762079007600640-8JYB) where we also disclose forward-looking product roadmap details.","The first link seems to describe an already released product rather than a forward-looking roadmap. Information disclosed at the AI Leaders Forums doesn't seem to be publicly available (e.g., it's not present in the provided LinkedIn post).",,0
Top distribution channels,Are the top-5 distribution channels for the model disclosed?,"We define distribution channels to be either an API provider (a pathway by which users can query the model with inputs and receive outputs) or a model distributor (a pathway by which model weights are released). We recognize that distribution channels may arise without the knowledge of the model developer. For example, the weights of a model may be released through one distribution channel and then be distributed through other channels.

Distribution channels can be ranked by any reasonable metric (e.g., number of queries, number of downloads, number of users, revenue). A description of the metric should be provided.

API providers and model distributors may be ranked separately using different metrics as long as the total number of distribution channels equals five (if five distribution channels exist). For example, the developer may choose to disclose the top-3 API providers (ranked by the number of queries) and the top-2 model distributors (ranked by the number of downloads).","We provide API access to the model through Writer API and Amazon Bedrock.

Please see https://aws.amazon.com/bedrock/writer/ and https://dev.writer.com/api-reference/api-keys#api-keys ","The developer specifies two distribution channels, which is sufficient for the top-5.",,1
Quantization,Is the quantization of the model served to customers in the top-5 distribution channels disclosed?,We will award this point for a disclosure of the model precision in each of the top-5 distribution channels. ,We serve the model at 16-bit precision on all distribution channels.,The developer discloses information about quantization for all distribution channels.,,1
Terms of use,Are the terms of use of the model disclosed?,"We define terms of use to include terms of service and model licenses. We will award this point for a pointer to the terms of service or model license.

In the event that model's licenses are written more generally, it should be clear which assets they apply to. We recognize that different developers may adopt different business models and therefore have different types of model licenses. Examples of model licenses include responsible AI licenses, open-source licenses, and licenses that allow for commercial use.

Terms of service should be disclosed for each of the top-5 distribution channels. However, we will award this point if there are terms-of-service that appear to apply to the bulk of the model’s distribution channels.","We publish our Platform Services Agreement (applicable to enterprise customers), our Terms of Use (applicable to individual users, and our open model license (applicable to any model we have ""open-sourced"") at https://writer.com/legal/. ",A terms of service that appears to apply to the bulk of the model's distribution channels is disclosed.,,1
Distribution channels with usage data,What are the top-5 distribution channels for which the developer has usage data?,"We define distribution channels to be either an API provider (a pathway by which users can query the model with inputs and receive outputs) or a model distributor (a pathway by which model weights are released). We recognize that distribution channels may arise without the knowledge of the model developer. For example, the weights of a model may be released through one distribution channel and then be distributed through other channels. 

Distribution channels can be ranked by any reasonable metric (e.g., number of queries, number of downloads, number of users, revenue). A description of the metric should be provided.


We define usage data as any form of developer-exclusive data collected from any of a developer's distribution channel. A developer has access to usage data from a distribution channel if it is able to use that data for downstream purposes (e.g., analytics, training etc.). Usage data may be shared outside of the developer, but it is initially collected by the distribution channel and shared to the developer.",,Company acknowledges no disclosure,,1
Amount of usage,"For each of the top-5 distribution channels, how much usage is there?","Usage should be reported as the number of queries over the span of a month, reported to the precision of one significant figure (e.g., 50 million queries).",No disclosure to company,Company acknowledges no disclosure,,1
Classification of usage data,"Is a representative, anonymized dataset classifying queries into usage categories disclosed?","Developers may either share a fully public dataset or a partially restricted dataset (e.g., under a research license). We will award this point if there is a clear, aggregated or sample dataset that reveals categories of tasks/queries.",No disclosure to company,Company acknowledges no disclosure,,1
Data retention and deletion policy,Is a policy for data retention and deletion disclosed?,"A data retention and deletion policy is a policy for removing particular data from the training set and/or preventing it from being used if there is a user or external request (e.g., “right to be forgotten”) that also covers internal data governance. This includes whether there is a formal process to delete or retract data from future training runs and how long raw data is retained. It also clarifies how quickly deletions propagate to the model (e.g., “only in subsequent major model releases”).",The model does not retain any user data. ,Develop states that the company does not retain any user data,,1
Geographic statistics,"Across all forms of downstream use, are statistics of model usage across geographies disclosed?","We will award this point if there is a meaningful, though potentially incomplete or vague, disclosure of geographic usage statistics at the country-level.",No disclosure to company,Company acknowledges no disclosure,,1
Internal products and services,What are the top-5 internal products or services using the model?,"An internal product or service is a product or service built by the developer. Products or services can be ranked by any reasonable metric (e.g., number of users, queries, revenue). A description of the metric should be provided.","Palmyra X5 is used to power the Writer platform, which includes 100+ prebuilt agents (Agent Library), Ask Writer,  and Agent Builder.

Please see writer.com ",Writer discloses each of the internal products and services that Palmyra X5,,1
External products and services,What are the top-5 external products or services using the model?,"An external product or service is a product or service built by a party external to the developer. Products or services can be ranked by any reasonable metric (e.g., number of users, queries, revenue). A description of the metric should be provided.

We will award a point if the developer discloses that that it does not have access to such metrics about external products or services.","Palmyra X5 is used for custom app and agent development via Amazon Bedrock.

Please see writer.com and https://aws.amazon.com/bedrock/writer/ 

In general, we do not have visibility into these metrics, specifically how our customers are using X5 (or the data they are sharing with us) beyond broad use case data and usage analytics. The main exception being when our customer success teams are engaged to assist with agent configuration, but in those cases, we are granted temporary access into customer instances of Writer to perform that configuration and do not access/collect customer data for metrics.","Writer discloses that external products and services are developed via Amazon Bedrock, and discloses that it doesn't have visibility into usage metrics beyond that. ",,1
Users of internal products and services,How many monthly active users are there for each of the top-5 internal products or services using the model?,"An internal product or service is a product or service built by the developer. The number of users refers to users who engaged or interacted with the model through the internal product or service over the last month or averaged over the last X months (this should be specified). Number of users should be specified to one significant figure (e.g. 100,000).","For the Writer platform, there are ~40k monthly active users.",Writer discloses the number of users of its platform,,1
Consumer/enterprise usage,"Across all distribution channels for which the developer has usage data, what portion of usage is consumer versus enterprise?","Consumer usage refers to usage by individual consumers. Enterprise usage refers to usage by enterprise customers (including government use). Consumer and enterprise usage should be calculated in terms of the number of queries by or the amount of revenue from consumer or enterprise users. Percentages should be specified to two significant digits (e.g., 12% consumer, 88% enterprise).",Approximately 75% are enterprises and the remainder is comprised of consumers.,Consumer vs enterprise usage is provided,,1
Enterprise users,"Across all distribution channels for which the developer has usage data, what are the top-5 enterprises that use the model?",Enterprises should be ranked by the number of queries made or the amount of revenue from usage since the model's release. We will also award this point if the developer indicates it does not have access to enterprise usage data.,"The top-5 enterprise users are: a global beauty brand, a global financial services company, a global consulting company, a global financial services company, and a global insurance company. ",Writer designates the category of the top 5 enterprise users,,1
Government use,What are the 5 largest government contracts for use of the model?,"This includes known government contracts of enterprise or government-specific products and services that use the model. We will award this point if the developer discloses its top five government contracts ranked monetary value, though the developer may omit contracts where it is under NDA regarding the existence of the contract.",We have an immaterial number of government users from US state governments and other non-US government entities.,Does not list top 5 government users. No information about which kinds of entities beyond state and non-US; no justification for why these entities are included or this information is not made available (e.g. NDAs),,0
Benefits Assessment,Is an assessment of the benefits of deploying the model disclosed?,We will award this point for any quantitative assessment of the benefits or potential benefits of deploying the model.,"We built a deep research agent using Palmyra X5 that evaluates and compares enterprise SaaS tools. In a standard evaluation of 5 product offerings, the agent reduced total research time by ~90%. 

Please see examples from our customers re ROI and benefits of use at https://writer.com/customers/ and https://writer.com/blog/roi-for-generative-ai/ ",The examples provide real-world estimates of benefits and ROI.,,1
AI bug bounty,Does the developer operate a public bug bounty or vulnerability reward program under which the model is in scope? ,"We will award this point for a publicly documented bug bounty or vulnerability reward program describing (i) in-scope vulnerabilities (e.g., prompt bypasses, data leaks), (ii) out-of-scope items, (iii) submission process, and (iv) reward tiers or recognition if applicable. We will award a point if the developer discloses it has no AI bug bounty that encourages external researchers to report security, privacy, or adversarial vulnerabilities in the model.","No, Writer does not currently operate a public bug bounty program.",No bug bounty,,1
Responsible disclosure policy,Does the developer clearly define a process by which external parties can disclose model vulnerabilities or flaws? ,"We will award this point for a description of the process external parties can use for responsbly disclosing model vulnerabilities and flaws, which should include (i) what mechanism external parties can use to disclose vulnerabilities or flaws (e.g., a form, an email) and (ii) what process follows a disclosure (e.g., how much time must parties wait until public release). This is often included with a bug bounty, but can also be standalone. We will award a point if the developer discloses it has no responsible disclosure policy.","Yes, we run a responsible vulnerability disclosure program (VDP) at https://writer.com/bugs/","VDP counts, says never disclose",,1
Safe harbor,Does the developer disclose its policy for legal action against external evaluators conducting good-faith research? ,"We will award this point if the developer discloses whether it has a policy committing it to not pursue legal action against external evaluators conducting good-faith research. This should not be only for software security vulnerabilities, but also AI flaws, and it should be based on researcher conduct standards, not at the sole discretion of the company. We will award this point if the developer provides a clear description of its policy regarding such protections for external researchers, or lack thereof. ","We do not have a policy with respect to taking legal action against third party researches, but as a practice, we do not take legal action against those parties provided they are working in good faith.","Says no safe harbor, but says they generally abide by one",,1
Security incident reporting protocol,Are major security incidents involving the model disclosed? ,"A security incident reporting protocol provides post-deployment transparency about serious incidents or breaches. Security incidents refer to incidents where external security threats affect the model (e.g., data breaches or DDoS attacks on the service). We will award this point if the developer states (i) how to submit a security incident report, (ii) how quickly it will respond, and (iii) when and whether results are disclosed.

Every incident need not be reported publicly, but the developer must disclose a policy determining how incidents are reported and disclosed.","Security incidents come in via our VDP program, details of which are at https://writer.com/bugs/.  Our disclosures are subject to all relevant legal and regulatory interests, described at https://writer.com/legal/",No information about the third requirement: public summaries of security incidents. Also unclear what the policy is due to the general referance to legal and regulatory requirements,,0
Misuse incident reporting protocol,Are misuse incidents involving the model disclosed? ,"A misuse incident reporting protocol provides post-deployment transparency about incidents of misuse involving the model. As opposed to the previous indicator, this indicator is about actors misusing the model to cause real-world harm, such as misinformation operations or cybersecurity attacks. We will award this point if the developer states (i) how to submit a misuse incident report, (ii) how quickly it will respond, and (iii) when and whether results are disclosed.

Every incident need not be reported publicly, but there needs to be a policy governing how incidents are reported.","In the Writer UI there is a ""Flag this output as inaccurate or inappropriate"" which users may use.  When responses are flagged, they go (anonymized) to a team who assesses and, if required, remediates the issue.

We do not currently publish a public incident report, that said, we have a practice of reviewing and actioning upon flagged content within a 30 day window after content is initially flagged in the way we described in our initial response, though this can occur more quickly depending on the level of sensitivity/nature of what has been flagged. To the extent a customer reaches our directly to our customer success team or to support@writer.com, those timelines may also decrease depending upon any agreed-upon SLAs we have in place with that customer.",All three requirements of the indicator are satisfied.,,1
Post-deployment coordination with government,Does the developer coordinate evaluation with government bodies?,"We will award this point if the developer specifies which government bodies it is coordinating with and for what types of post-deployment evaluations. Government bodies include AI Safety Institutes, national security agencies, national labs, and international governmental enties such as UN agencies or the G7. Evaluation here may also include sharing of the developer's proprietary evaluation results for help with interpretation. ","We are memberrs of, and coordinate with, IT-ISAC and FBI's InfraGard for threat intelligence and incident collaboration",Provides government entites that they coordinate with,,1
Feedback mechanisms,"Does the developer disclose a way to submit user feedback? If so, is a summary of major categories of feedback disclosed?","We will award this point if the developer (i) discloses how users can submit feedback (e.g., via a form or a thumbs up/thumbs down for model responses) and (ii) discloses aggregated or categorized feedback data (e.g. a categorization of thumbs up and thumbs down data). ","A user can submit feedback through a number of channels: (1) within the Writer platform, a user can flag output as either inappropriate or inaccurate; and (2) a user can share their concerns by reaching out to support@writer.com, legal@writer.com, and/or privacy@writer.com",Provides several feedback channels,,1
"Permitted, restricted, and prohibited model behaviors","Are model behaviors that are permitted, restricted, and prohibited disclosed?","We refer to a policy that includes this information as a model behavior policy, or a developer's policy on what the foundation model can and cannot do (e.g. such a policy may prohibit a model from responding to NSFW content). We recognize that different developers may adopt different business models and that some business models may make enforcement of a model behavior policy more or less feasible. We will award this point if at least two of the three categories (i.e. permitted, restricted, and prohibited model behaviors) are disclosed. Alternatively, we will award this point if the developer reports that it does not impose any restrictions on its model's behavior in this way.","Please see our acceptable use policy available at https://writer.com/legal/acceptable-use/ which restricts how our models, including Palmyra X5, may be used.

We permit users to input a broad array of questions and requests into X5, but we have technical measures in place, as described at https://dev.writer.com/home/usage-policy and https://dev.writer.com/home/toxic-check, which are examples of how we help to enforce the retrictions we impose via our acceptable use policy from a technical pov.","This is a restriction on how users can make use of the model, not what the model can do.",,0
Model response characteristics,Are desired model response characteristics disclosed?,"Model response characteristics include default behaviors or behaviors that the developer steers the model to take. These may include being helpful, taking an objective point of view, or using tools only when necessary. We will award points for a clear description of desired model response characteristics or a statement that there are no such characteristics.","Palmyra X5 model responses are intended to provide high-quality, useful, and reliable assistance. The model is instructed to take on a role as an intelligent assistant to humans.

","The desired model response characteristics are high-quality, useful, and reliable assistance to humans.",,1
System prompt,Is the default system prompt for at least one distribution channel disclosed?,A system prompt is defined as the prompt provided to the system by default that guides the system's behavior. We will award this point for the disclosure of the verbatim text of the full system prompt as well as an explanation for the context in which the system prompt is used.,We do not disclose our default prompt for Palmyra X5. ,Company acknowledges no disclosure,,0
Intermediate tokens,Are intermediate tokens used to generate model outputs available to end users? ,"Intermediate tokens are defined as any tokens generated by the model before the final output is shown to the user, such as model chains of thought. We will also award this point if a summary of intermediate tokens is made available to end users. If intermediate tokens or summaries are not made available, the developer should provide a justification.","We provide explainable outputs with source-citing and chain-of-thought features. We also believe it is imperative that our users know what is happening to their data when they interact with Palmyra X5, which is why we make it clear that we have not trained Palmyra X5 (or any of our models for that matter) on user data, including inputs or output content.
",Chains of thought disclosure suffices,,1
Internal product and service mitigations,"For internal products or services using the model, are downstream mitigations against adversarial attacks disclosed?","An internal product or service is a product or service built by the developer. Adversarial attacks include prompt injection, jailbreaking, or malicious queries. Mitigations against adversarial attacks might include specialized prompt filtering, content scanning, or real-time monitoring of queries or accounts. We will award this point if the developer discloses a clear statement of methods used (e.g., a specialized prompt sanitizer or adversarial pattern detector), or if the developer states it does not implement such product-level mitigations against adversarial attacks.",Our mitigations are documented at https://dev.writer.com/home/prompt_injections,Disclosure of mitigations suffices,,1
External developer mitigations,Does the developer provide built-in or recommended mitigations against adversarial attacks for downstream developers?,"Downstream developers are developers who access the model through a distribution channel. Adversarial attacks include prompt injection, jailbreaking, or malicious queries. Mitigations against adversarial attacks that developers might build in or recommend include content filtering endpoints and recommended prompt templates. We will award this point if the developer discloses (i) technical mitigations (e.g., a developer provided moderation API or classifier) it offers or implements, (ii) recommended best practices or libraries for downstream developers, or (iii) an explicit statement that it does not build or recommend any particular downstream mitigations in this way.. ","Our API gateway performs rate limiting and our platform always defends against prompt injection attacks.

For additional details, please see https://dev.writer.com/home/prompt_injections, specifically the sections covering measures against prompt injections, safeguards against jailbreak attacks, and measures against secrets leakage. ",Mitigations for external developers are listed clearly.,,1
Enterprise mitigations,Does the developer disclose additional or specialized mitigations for enterprise users?,"Enterprise users are, for example, large organizations with dedicated service agreements or users of enterprise-specific API deployments or products and services. Additional or specialized mitigations may address enterprise needs such as data privacy controls, advanced prompt/response monitoring, or compliance checks with regulations such as GDPR or HIPAA. Additional or specialized mitigations may include single-tenant deployments, custom filters for specific regulated industries, or advanced logging for compliance. We will award a point if the developer at least describes these mitigations or states that it does not provide such additional or specialized enterprise mitigations.","Writer is a multi-tenant generative AI platform and is availble as feature-equivalent single-tenant deployments for enterprise customers.

In addition to managed deployment, enterprise users of the Writer platform benefit from admin controls (e.g. session logging, user roles & permissions), authentication and access controls (e.g. SSO, MFA), and additional security and privacy features (e.g. activity monitoring, data encryption). 

Enterprise users can also access Palmyra X5 via Amazon Bedrock, taking advantage of AWS’ serverless, fully managed infrastructure while maintaining full control of their data with encryption in transit and at rest, identity-based policies, and more.",Describes security controls,,1
Detection of machine-generated content,Are mechanisms that are used for detecting content generated by this model disclosed?,"A mechanism for detecting machine-generated content might include storing a copy of all outputs generated by the model to compare against, implementing a watermark on model outputs, adding cryptographic metadata (such as C2PA), or training a detector post-hoc to identify such content. We will award this point if any such mechanism is disclosed or if the developer reports that it does not have or use any such mechanism.","At the time of Palmyra-X5’s release, no watermarking or cryptographic tagging mechanism is applied to outputs. Given the model’s primary deployment in controlled enterprise environments, where usage is authenticated and logged, our risk assessment determined that real-time content detection mechanisms were not a priority for this version.

",Developer discloses no watermarking,,1
Documentation for responsible use,Does the developer provide documentation for responsible use by downstream developers?,"To receive a point, the developer should provide documentation for responsible use. This might include details on how to adjust API settings to promote responsible use, descriptions of how to implement mitigations, or guidelines for responsible use. We will also award this point if the developer states that it does not provide any such documentation. For example, the developer might state that the model is offered as is and downstream developers are accountable for using the model responsibly.","Palmyra X5 is offered as is and downstream users are accountable for using the model responsibly, subject to Writer's acceptable use policy, available https://writer.com/legal/acceptable-use/.",Writer acknowledges they do not provide documentation for responsible use by downstream developers.,,1
Permitted and prohibited users,Is a description of who can and cannot use the model on the top-5 distribution channels disclosed?,We will award this point for a description of the company's policies for permitted and prohibitted users on its top-5 distribution channels. We will award this point if the developer has a more general acceptable use policy that it confirms applies across these distribution channels. We will award this point if there are no restrictions on users.,"Writer maintains an acceptable use policy (available at https://writer.com/legal/acceptable-use/ that applies to any use using Writer's platform. We also do not permit users below the age 16 to use our platform, as outlined in our privacy policy: https://writer.com/legal/privacy/ and our terms of use, available at https://writer.com/legal/terms-of-use/. ",Does not permit children to use the product,,1
"Permitted, restricted, and prohibited uses","Which uses are explicitly allowed, conditionally permitted, or strictly disallowed under the acceptable use policy for the top-5 distribution channels?","We will award this point for a rough characterization of two or more of permitted, restricted, and prohibited uses across the top-5 distribution channels. We will award this point if the developer has a more general acceptable use policy that it confirms applies across these distribution channels. We will award this point if there are no restrictions on users.","We do not restrict usage with the exception of what we prohibit in our acceptable use policy, available at https://writer.com/legal/acceptable-use/ ","AUP clarifies the uses, this suffices",,1
AUP enforcement process,What are the methods used by the developer to enforce the acceptable policy?,"We will award this point if the developer discloses the processes (automated or manual) it uses to detect, review, and respond to potential acceptable use policy violations. We will award this point for a reasonable best-effort attempt to provide the bulk of this information, though one line indicating the developer reserves the right to terminate accounts is insufficient. Alternatively, we will award this point if the developer reports that it does not use such methods to enforce its acceptable use policy.","We use a combination of human review and automated checks to enforce our Acceptable Use Policy

You can find additional details about how we conduct automated checks to help enforce our AUP at https://dev.writer.com/home/toxic-check in addition to our response to Question 85.",Describes AUP enforcement process,,1
AUP enforcement frequency,Are statistics on the developer's AUP enforcement disclosed?,"We will award this point if the developer discloses enforcement statistics (e.g., violation counts or actions taken) from its enforcement of its acceptable use policy. Alternatively, we will award this point if the developer reports that it does not enforce its acceptable use policy.","While we enforce our Acceptable Use Policy, we do not currently publish any accompanying report about this enforcement.",Company acknowledges no disclosure,,0
Regional policy variations,Are differences in the developer's acceptable use or model behavior policy across geographic regions disclosed?,"We will award this point if the developer discloses distinctions in its AUP or MBP and provides examples of differences in multiple specific regions, or states that no differences exist. For example, some jurisdictions impose content restrictions beyond those in the developer’s global policy that may necessesitate local deviations.","No differences exist, we have one uniform acceptable use policy, available at https://writer.com/legal/acceptable-use/ ",No discrepancies by region,,1
Oversight mechanism,Does the developer have an internal or external body that reviews core issues regarding the model prior to deployment?,"We will award this point if the developer discloses that is has such an internal or external body and provides some description of its scope, or alternatively if the developer discloses that it has no such body. An oversight mechanism covers governance structure beyond mere external risk evaluation, asking whether a formal body regularly reviews design and deployment decisions. Core issues may include model objectives, data usage, or risk mitigation.","We convene a quarterly internal compliance committee meeting to review risks related to our platform, including our model development process, from a security, privacy, and compliance perspective. This meeting is attended by stakeholders from across the company, including our AI Research team, security team, privacy team, and legal team.",Internal body disclosed,,1
Whistleblower protection,Does the developer disclose a whistleblower protection policy?,"We will award this point if the developer discloses (i) the existence of a whistleblower protection policy, (ii) what protections are afforded to whistleblowers, (iii) how reports are handled and investigated, and (iv) any external oversight of the whistleblower protection process. This might include protections for whistleblowers who report safety, ethical, or legal concerns related to the model. We will also award this point if the developer discloses that it has no such policy.",We work with a third party (Miratech) for anonymous reporting (https://report.syntrio.com/writer) across a broad spectrum of issues. We have codified this ability to report in our Employee Handbook.,"Whistleblower protection disclosed, but specifics of policy in handbook not disclosed",,0
Government commitments,What commitments has the developer made to government bodies?,We will award this point if the company provides an exhaustive list of commitments it has made to government bodies in the jurisdictions where it offers its model.,We make a broad commitment to comply with all applicable global AI laws and regulations via this public statement: https://writer.com/legal/global-ai-regulation/. We have also voluntarily committed to joining the AI Pact (as of the fall of 2024). ,AI Pact commitment disclosed,,1
