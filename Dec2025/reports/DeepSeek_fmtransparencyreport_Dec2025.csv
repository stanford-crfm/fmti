Indicator,Definition,Notes,Disclosure
Data acquisition methods,What methods does the developer use to acquire data used to build the model?,"Which of the following data acquisition methods does the developer use: â€¨
(i) acquiring existing public datasets,
(ii) crawling the web,
(iii) using data acquired via its existing products and services,
(iv) licensing existing data from external parties,
(v) having humans create or annotate new data, 
(vi) using models to generate new data,â€¨ or
(vii) other data acquisition methods not captured by the above.

For example, if the developer uses reinforcement learning from human feedback to train models using model-generated outputs with human preference annotations, this would satisfy categories (v) and (vi).

Alternatively, if the developer post-trains its model using off-the-shelf preference data (for example, the Alpaca dataset), this would satisfy category (i).","""For the training data of DeepSeek-V3-Base, we exclusively use plain web pages and
e-books, without incorporating any synthetic data. ... Notably, DeepSeek-R1 and DeepSeek-R1-Zero are trained
on top of DeepSeek-V3-Base and DeepSeek-R1 leverages non-reasoning data from
DeepSeek-V3 SFT data.""

""Public Data: We use publicly available information on the internet to build the model's broad understanding of world knowledge. We employ technical methods to acquire and filter these freely accessible data to enrich the model's knowledge base.
Licensed Data: We collaborate with third-party data providers to obtain proprietary datasets through legally signed agreements. We ensure all collaborations are based on lawful authorization."" For the training data of DeepSeek-V3-Base, we exclusively use plain web pages and
e-books, without incorporating any synthetic data. However, we have observed that
some web pages contain a significant number of OpenAI-model-generated answers,
which may lead the base model to acquire knowledge from other powerful models
indirectly. However, we did not intentionally include synthetic data generated by
OpenAI during the pre-training cooldown phase; all data used in this phase were
naturally occurring and collected through web crawling."
Public datasets,What are the top-5 sources (by volume) of publicly available datasets acquired for building the model?,We define a source as the entity or means by which the developer acquires data. We define the top-5 sources as the top-5 sources by data volume. ,No information provided about data acquisition.
Crawling,"If data collection involves web-crawling, what is the crawler name and opt-out protocol?","We award this point for disclosure of the crawler name and opt-out protocols, including if/how they respect the Robots Exclusion Protocol (robots.txt).","No information provided about data crawling, though some additional tangential information: """"DeepSeek-LLM documentation mentions 'self-collected data respecting robots.txt' but does not specify crawler name (URL: https://github.com/deepseek-ai/DeepSeek-LLM)
GitHub issue #53 specifically asks about crawler name but receives no response, indicating it is not documented (URL: https://github.com/deepseek-ai/DeepSeek-LLM/issues/53)
DeepSeek-Math uses data 'sourced from Common Crawl' with FastText model for retrieval (URL: https://github.com/deepseek-ai/DeepSeek-Math)
DeepSeek-R1 is based on DeepSeek-V3-Base which uses 14.8 trillion tokens from diverse sources (URL: https://huggingface.co/deepseek-ai/DeepSeek-R1)
Terms of Service prohibit users from using robots/spiders but does not disclose their own crawler practices (URL: https://cdn.deepseek.com/policies/en-US/deepseek-terms-of-use.html)"""""
Usage data used in training,What are the top-5 sources (by volume) of usage data from the developer's products and services that are used for building the model?,We define usage data as data collected from the use of a developer's products or services.,"""Personal Data You Provide
When you create an account, input content, contact us directly, or otherwise use the Services, you may provide some or all of the following Personal Data:

Account Personal Data. We collect Personal Data that you provide when you set up an account, such as your date of birth (where applicable), username (where applicable), email address and/or telephone number, and password. 
User Input. When you use our Services, we may collect your text input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services (""Prompts"" or ""Inputs""). We generate responses (""Outputs"") based on your Inputs.
Personal Data When You Contact Us. When you contact us, we collect the Personal Data you send us, such as proof of identity or age, contact details, feedback or inquiries about your use of the Services or Personal Data about possible violations of our Terms of Service (our ""Terms"") or other policies.
Automatically Collected Personal Data
We automatically collect certain Personal Data from you when you use the Services, including internet or other network activity Personal Data such as your IP address, unique device identifiers, and cookies.

Device and Network Personal Data. We collect certain device and network connection Personal Data when you access the Services. This Personal Data includes your device model, operating system, IP address, device identifiers and system language. We also collect service-related, diagnostic, and performance Personal Data, including crash reports and performance logs. We automatically assign you a device ID and user ID. Where you log-in from multiple devices, we use Personal Data such as your device ID and user ID to identify your activity across devices to give you a seamless log-in experience and for security purposes.
Log Personal Data. We collect Personal Data regarding your use of the Services, such as the features you use and the actions you take.
Location Personal Data. We automatically collect Personal Data about your approximate location based on IP address for security reasons, for example to protect your account by detecting unusual login activity.
Cookies & Similar Technologies. We may use cookies and similar tracking technologies to operate and provide the Service. For example, we use cookies for security purposes and to better understand how the Service is used. We will allow you to manage  our use of non-essential cookies where required by law. To learn more about our use of cookies, please see our Cookies Policy .
Payment Personal Data. When you use paid services for prepayment, we collect your payment order and transaction Personal Data to provide Services such as order placement, payment, customer service, and after-sales support."""
Notice of usage data used in training,"For the top-5 sources of usage data, how are users of these products and services made aware that this data is used for building the model?","We define usage data notice as the proactive disclosure to users of how their data is used for model development. For example, via a pop-up with a description, a link to the privacy policy, or link to a description of company practices. ","""Personal Data You Provide
When you create an account, input content, contact us directly, or otherwise use the Services, you may provide some or all of the following Personal Data:

Account Personal Data. We collect Personal Data that you provide when you set up an account, such as your date of birth (where applicable), username (where applicable), email address and/or telephone number, and password. 
User Input. When you use our Services, we may collect your text input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services (""Prompts"" or ""Inputs""). We generate responses (""Outputs"") based on your Inputs.
Personal Data When You Contact Us. When you contact us, we collect the Personal Data you send us, such as proof of identity or age, contact details, feedback or inquiries about your use of the Services or Personal Data about possible violations of our Terms of Service (our ""Terms"") or other policies.
Automatically Collected Personal Data
We automatically collect certain Personal Data from you when you use the Services, including internet or other network activity Personal Data such as your IP address, unique device identifiers, and cookies.

Device and Network Personal Data. We collect certain device and network connection Personal Data when you access the Services. This Personal Data includes your device model, operating system, IP address, device identifiers and system language. We also collect service-related, diagnostic, and performance Personal Data, including crash reports and performance logs. We automatically assign you a device ID and user ID. Where you log-in from multiple devices, we use Personal Data such as your device ID and user ID to identify your activity across devices to give you a seamless log-in experience and for security purposes.
Log Personal Data. We collect Personal Data regarding your use of the Services, such as the features you use and the actions you take.
Location Personal Data. We automatically collect Personal Data about your approximate location based on IP address for security reasons, for example to protect your account by detecting unusual login activity.
Cookies & Similar Technologies. We may use cookies and similar tracking technologies to operate and provide the Service. For example, we use cookies for security purposes and to better understand how the Service is used. We will allow you to manage  our use of non-essential cookies where required by law. To learn more about our use of cookies, please see our Cookies Policy .
Payment Personal Data. When you use paid services for prepayment, we collect your payment order and transaction Personal Data to provide Services such as order placement, payment, customer service, and after-sales support."""
Licensed data sources,What are the top-5 sources (by volume) of licensed data acquired for building the model?,"We define a source as the entity from which the developer acquires data. For example, the Associated Press is reportedly a source of licensed data for OpenAI.",No information provided about data acquisition.
Licensed data compensation,"For each of the top-5 sources of licensed data, are details related to compensation disclosed?",We award this point if the model developer describes the compensation structure specified in the contract with the data source or indicates they are prohibited from sharing this information if contractually mandated.,No information provided about data acquisition.
New human-generated data sources,What are the top-5 sources (by volume) of new human-generated data for building the model?,"We define a source as the entity or means by which the developer acquires data. For example, Scale AI could be a source of new human-generated data. By new, we mean the data is specifically acquired for the purposes of building the model.",No information provided about data acquisition.
Instructions for data generation,"For each of the top-5 sources of human-generated data, what instructions does the developer provide for data generation?","The instructions should be those provided to the data source. For example, if a third-party vendor works directly with the data laborers to produce the data, the instructions from the developer to this vendor should be disclosed.",No information provided about data acquisition.
Data laborer practices,"For the top-5 sources of human-generated data, how are laborers compensated, where are they located, and what labor protections are in place?","For each data source, we require (i) the compensation in either USD or the local currency, (ii) any countries where at least 25% of the laborers are located, and (iii) a description of any labor protections. We will award this point if the developer discloses that it is not aware of data laborer practices.",No information provided about data acquisition.
Synthetic data sources,What are the top-5 sources (by volume) of synthetic data acquired for building the model?,We define a source of synthetic data as a non-human mechanism (e.g. a machine learning model) used to generate the data.,"""2.3.1. Cold Start
Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from
the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data
to fine-tune the model as the initial RL actor. To collect such data, we have explored several
approaches: using few-shot prompting with a long CoT as an example, directly prompting
models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-
Zero outputs in a readable format, and refining the results through post-processing by human
annotators.
In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as
the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data
9
include:
â€¢ Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable
for reading. Responses may mix multiple languages or lack markdown formatting to
highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,
we design a readable pattern that includes a summary at the end of each response and
filters out responses that are not reader-friendly. Here, we define the output format as
|special_token|<reasoning_process>|special_token|<summary>, where the reasoning
process is the CoT for the query, and the summary is used to summarize the reasoning
results.
â€¢ Potential: By carefully designing the pattern for cold-start data with human priors, we
observe better performance against DeepSeek-R1-Zero. We believe the iterative training is
a better way for reasoning models."""
Synthetic data purpose,"For the top-5 sources of synthetically generated data, what is the primary purpose for data generation?",We define a source of synthetic data as a non-human mechanism (e.g. a machine learning model) used to generate the data.,"""2.3.1. Cold Start
Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from
the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data
to fine-tune the model as the initial RL actor. To collect such data, we have explored several
approaches: using few-shot prompting with a long CoT as an example, directly prompting
models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-
Zero outputs in a readable format, and refining the results through post-processing by human
annotators.
In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as
the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data
9
include:
â€¢ Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable
for reading. Responses may mix multiple languages or lack markdown formatting to
highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,
we design a readable pattern that includes a summary at the end of each response and
filters out responses that are not reader-friendly. Here, we define the output format as
|special_token|<reasoning_process>|special_token|<summary>, where the reasoning
process is the CoT for the query, and the summary is used to summarize the reasoning
results.
â€¢ Potential: By carefully designing the pattern for cold-start data with human priors, we
observe better performance against DeepSeek-R1-Zero. We believe the iterative training is
a better way for reasoning models."""
Data processing methods,What are the methods the developer uses to process acquired data to determine the data directly used in building the model?,"We will award this point for disclosure of all of the methods used to process acquired data. Data processing refers to any method that substantively changes the content of the data. For example, compression or changing the data file format is generally not in the scope of this indicator.","We exclude sensitive information, credit card numbers, or unique identification information from our training data sources to minimize the risk of collecting any personal information. However, due to the vast scale of pre-training data, some publicly available online content or licensed data from other providers may incidentally contain personal information. We employ technical measures to screen and remove such information from the training data as much as possible and conduct tests before using the data for training.

Additionally, to ensure data quality, safety, and diversity, we have established a rigorous data governance process. First, we use filters to automatically screen and remove raw data containing hate speech, pornography, violence, spam, or potential infringement. Second, recognizing that large-scale datasets may inherently contain statistical biases, we combine algorithmic and manual review methods to identify and mitigate the impact of these biases on the model's values, thereby enhancing fairness."
Data processing purpose,"For each data processing method, what is its primary purpose?","Data processing refers to any method that substantively changes the content of the data. For example, compression or changing the data file format is generally not in the scope of this indicator.","We exclude sensitive information, credit card numbers, or unique identification information from our training data sources to minimize the risk of collecting any personal information. However, due to the vast scale of pre-training data, some publicly available online content or licensed data from other providers may incidentally contain personal information. We employ technical measures to screen and remove such information from the training data as much as possible and conduct tests before using the data for training.

Additionally, to ensure data quality, safety, and diversity, we have established a rigorous data governance process. First, we use filters to automatically screen and remove raw data containing hate speech, pornography, violence, spam, or potential infringement. Second, recognizing that large-scale datasets may inherently contain statistical biases, we combine algorithmic and manual review methods to identify and mitigate the impact of these biases on the model's values, thereby enhancing fairness."
Data processing techniques,"For each data processing method, how does the developer implement the method?","Data processing refers to any method that substantively changes the content of the data. For example, compression or changing the data file format is generally not in the scope of this indicator.","We exclude sensitive information, credit card numbers, or unique identification information from our training data sources to minimize the risk of collecting any personal information. However, due to the vast scale of pre-training data, some publicly available online content or licensed data from other providers may incidentally contain personal information. We employ technical measures to screen and remove such information from the training data as much as possible and conduct tests before using the data for training.

Additionally, to ensure data quality, safety, and diversity, we have established a rigorous data governance process. First, we use filters to automatically screen and remove raw data containing hate speech, pornography, violence, spam, or potential infringement. Second, recognizing that large-scale datasets may inherently contain statistical biases, we combine algorithmic and manual review methods to identify and mitigate the impact of these biases on the model's values, thereby enhancing fairness."
Data size,Is the size of the data used in building the model disclosed?,"To receive this point, the developer should report data size in appropriate units (e.g. bytes, words, tokens, images, frames) and broken down by modality. Data size should be reported to a precision of one significant figure (e.g. 4 trillion tokens, 200 thousand images). The size should reflect data directly used in building the model (i.e. training data) and not data that was acquired but unused, or data used to evaluate the model.","From the DeepSeek-V3 technical report: ""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of
DeepSeek-V3 on 14.8T tokens"". Nature paper for R1 notes scale of training data for going from V3 to R1 (e.g. 800k SFT samples of average length of 5k tokens, hence 4B tokens)"
Data language composition,"For all text data used in building the model, what is the composition of languages?","To receive this point, the developer should report (i) all languages which make up at least 1% of the data and their corresponding proportions and (ii) a brief description of how languages are labeled (if a publicly available tool is used, include a link to the tool). Proportions should be reported to a precision of two significant figures and should describe proportions of documents labeled with some langauge. An ""Unknown"" category may be included to denote documents where the language could not be identified.","From the DeepSeek-V3 technical report: ""we optimize the pre-training corpus by enhancing the ratio
of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."""
Data domain composition,"For all the data used in building the model, what is the composition of domains covered in the data?","To receive this point, the developer should report the composition of the main domains included in the data used to train the model. This data should be at a level of granularity lower than broad claims about training on ""internet data"". For example, this could include the proportion of data from e-commerce, social media, news, code, etc. based on the URLs from which the data is sourced. Proportions should be reported to a precision of one significant figure.","The most relevant document is the DeepSeek-V3 technical report with the relevant passage on data domains stating: ""Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio
of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."" Further information on post-training states: ""We curate our instruction-tuning datasets to include 1.5M instances spanning multiple domains,
with each domain employing distinct data creation methods tailored to its specific requirements.
Reasoning Data. For reasoning-related datasets, including those focused on mathematics,
code competition problems, and logic puzzles, we generate the data by leveraging an internal
DeepSeek-R1 model. Specifically, while the R1-generated data demonstrates strong accuracy, it
suffers from issues such as overthinking, poor formatting, and excessive length. Our objective is
to balance the high accuracy of R1-generated reasoning data and the clarity and conciseness of
regularly formatted reasoning data.
To establish our methodology, we begin by developing an expert model tailored to a specific
domain, such as code, mathematics, or general reasoning, using a combined Supervised FineTuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as a
data generator for the final model. The training process involves generating two distinct types
of SFT samples for each instance: the first couples the problem with its original response in
the format of <problem, original response>, while the second incorporates a system prompt alongside the problem and the R1 response in the format of <system prompt, problem, R1
response>. The system prompt is meticulously designed to include instructions that guide the model
toward producing responses enriched with mechanisms for reflection and verification. During
the RL phase, the model leverages high-temperature sampling to generate responses that
integrate patterns from both the R1-generated and original data, even in the absence of explicit
system prompts. After hundreds of RL steps, the intermediate RL model learns to incorporate
R1 patterns, thereby enhancing overall performance strategically.
Upon completing the RL training phase, we implement rejection sampling to curate highquality SFT data for the final model, where the expert models are used as data generation
sources. This method ensures that the final training data retains the strengths of DeepSeek-R1
while producing responses that are concise and effective. No further information is provided about the post-training involved in producing DeepSeek-R1."""
External data access,Does a third-party have direct access to the data used to build the model?,"By a third-party, we mean entities that are financially independent of the developer.  We will award this point if at least one such entity is named as having direct access to the data. With that said, we may award this point if the developer provides justifications for prohibiting access to narrowly-scoped parts of the data. ",No information provided about external data access.
Data replicability,Is the data used to build the model described in enough detail to be externally replicable?,We will award this point if the description contains (i) a list of all publicly available training data and where to obtain it and (ii) a list of all training data obtainable from third parties and where to obtain it. These conditions refer to criteria 2 and 3 under the OSI Open Source AI v1.0 definition.,No information provided relevant for data replication.
Compute usage for final training run,Is the amount of compute used in the model's final training run disclosed?,"Compute should be reported in appropriate units, which most often will be floating point operations (FLOPs), along with a description of the measurement methodology, which may involve estimation. Compute should be reported to a precision of one significant figure (e.g. 5 x 10^25 FLOPs). This number should represent the compute used to train the final model across all model stages.","No information provided about training FLOPs, though it can be estimated using published GPU hours of 2.8M H800 hours in DeepSeek-V3 paper by assuming estimates for conversion factors and information about R1 RL phase in R1 paper."
Compute usage including R&D,"Is the amount of compute used to build the model, including experiments, disclosed?","Compute should be reported in appropriate units, which most often will be floating point operations (FLOPs), along with a description of the measurement methodology, which may involve estimation. Compute should be reported to a precision of one significant figure (e.g. 7 x 10^26 FLOPs). Compared to the previous indicator, this indicator should include an estimation of the total compute used across experiments used towards the final training run for the model (such as including hyperparameter optimization or other experiments), and not just the final training run itself.",No information provided about cumulative training FLOPs.
Development duration for final training run,Is the amount of time required to build the model disclosed?,"The amount of time should be specified in terms of both the continuous duration of time required and the number of hardware hours used. The continuous duration of time required to build the model should be reported in weeks, days, or hours to a precision of one significant figure (e.g. 3 weeks). The number of hardware hours should be reported to a precision of one significant figure and include the type of hardware hours. No form of decomposition into phases of building the model is required for this indicator, but it should be clear what the duration refers to (e.g. training the model, or training and subsequent evaluation and red teaming).","The most relevant document is the DeepSeek-V3 paper with the relevant passage on pre-training: ""During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K
H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined
with 119K GPU hours for the context length extension and 5K GPU hours for post-training,
DeepSeek-V3 costs only 2.788M GPU hours for its full training"". Nature paper on R1 notes: For the training of DeepSeek-R1-Zero, we employed 64*8 H800 GPUs,
and the process required approximately 198 hours. Additionally, during the training
phase of DeepSeek-R1, we utilized the same 64*8 H800 GPUs, completing the process
in about 4 days, or roughly 80 hours. To create the SFT datasets, we use 5K GPU
hours"
Compute hardware for final training run,"For the primary hardware used to build the model, is the amount and type of hardware disclosed?","In most cases, this indicator will be satisfied by information regarding the number and type of GPUs or TPUs used to train the model. The number of hardware units should be reported to a precision of one significant figure (e.g. 800 NVIDIA H100 GPUs). We will not award this point if (i) the training hardware generally used by the developer is disclosed, but the specific hardware for the given model is not, or (ii) the training hardware is disclosed, but the amount of hardware is not. We will award this point even if information about the interconnects between hardware units is not disclosed.","The most relevant document is the DeepSeek-V3 paper with the relevant passage on pre-training: ""During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K
H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined
with 119K GPU hours for the context length extension and 5K GPU hours for post-training,
DeepSeek-V3 costs only 2.788M GPU hours for its full training"". No additional information is provided on the R1 phase."
Compute provider,Is the compute provider disclosed?,"For example, the compute provider may be the model developer in the case of a self-owned cluster, a cloud provider like Microsoft Azure, Google Cloud Platform, or Amazon Web Services, or a national supercomputer. In the event that compute is provided by multiple sources or is highly decentralized, we will award this point if a developer makes a reasonable effort to describe the distribution of hardware owners.","The most relevant document is the DeepSeek-V3 paper with the relevant passage on pre-training: ""During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K
H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined
with 119K GPU hours for the context length extension and 5K GPU hours for post-training,
DeepSeek-V3 costs only 2.788M GPU hours for its full training"". No additional information is provided on the R1 phase."
Energy usage for final training run,Is the amount of energy expended in building the model disclosed?,"Energy usage should be reported in appropriate units, which most often will be megawatt-hours (mWh), along with a description of the measurement methodology, which may involve estimation. Energy usage should be reported to a precision of one significant figure (e.g. 500 mWh). No form of decomposition into compute phases is required, but it should be clear whether the reported energy usage is for a single model run or includes additional runs, or hyperparameter tuning, or training other models like reward models, or other steps in the model development process that necessitate energy usage. If the developer is unable to measure or estimate this quantity due to information not being available from another party (e.g. compute provider), we will award this point if the developer explicitly discloses what information it lacks and why it lacks it.",No information provided about training energy usage.
Carbon emissions for final training run,Is the amount of carbon emitted in building the model disclosed?,"Emissions should be reported in appropriate units, which most often will be tons of carbon dioxide emitted (tCO2), along with a description of the measurement methodology, which may involve estimation. Emissions should be reported to a precision of one significant figure (e.g. 500 tCO2). No form of decomposition into compute phases is required, but it should be clear whether the reported emissions is for a single model run or includes additional runs, or hyperparameter tuning, or training other models like reward models, or other steps in the model development process that generate emissions. If the developer is unable to measure or estimate this quantity due to information not being available from another party (e.g. compute provider), we will award this point if the developer explicitly discloses what information it lack and why it lacks it. Emissions should correspond with the energy used in the previous indicator.",No information provided about environmental impacts.
Water usage for final training run,Is the amount of clean water used in building the model disclosed?,"Clean water usage should be in appropriate units, which most often will be megaliters, along with a description of the measurement methodology, which may involve estimation. Clean water usage should be reported to a precision of one significant figure (e.g., 5000ML). No form of decomposition into compute phases is required, but it should be clear whether the reported water usage is for a single model run or includes additional runs, or hyperparameter tuning, or training other models like reward models, or other steps in the model development process that necessitates water usage. If the developer is unable to measure or estimate this quantity due to information not being available from another party (e.g. compute provider), we will award this point if the developer explicitly discloses what information it lacks and why it lacks it.",No information provided about environmental impacts.
Internal compute allocation,How is compute allocated across the teams building and working to release the model?,"To receive a point, the developer should provide the compute allocated to each team involved in training the model. We understand there might be no clear allocation of compute across different teams; in that case, report an estimate of the compute used over the last year. Compute allocation should be reported to at least one significant figure.","The most relevant document is the DeepSeek-V3 paper with the relevant passage on compute allocation: ""2664K H800 hours on pretraining, 119K on context extension, 5k on post training"". Nature paper provides compute allocation of 100k H800 GPU hours for R1-Zero, 5k for SFT data creation, and 41k for R1."
Model stages,Are all stages in the model development process disclosed?,"Stages refer to each identifiable step that constitutes a substantive change to the model during the model building process. We recognize that different developers may use different terminology for these stages, or conceptualize the stages differently. We will award this point if there is a clear and complete description of these stages.","The most relevant documents are the DeepSeek-V3 and DeepSeek-R1 technical reports. The relevant passages state: ""During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The
pre-training process is remarkably stable. Throughout the entire training process, we did not
encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage
context length extension for DeepSeek-V3. In the first stage, the maximum context length is
extended to 32K, and in the second stage, it is further extended to 128K. Following this, we
conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)
on the base model of DeepSeek-V3, to align it with human preferences and further unlock its
potential"" to build V3. Then, ""To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training
pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the
DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-
Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection
sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains
such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.
After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking
into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to
as DeepSeek-R1""."
Model objectives,"For all stages that are described, is there a clear description of the associated learning objectives or a clear characterization of the nature of this update to the model?","We recognize that different developers may use different terminology for these stages, or conceptualize the stages differently. We will award this point if there is a clear description of the update to the model related to each stage, whether that is the intent of the stage (e.g. making the model less harmful), a mechanistic characterization (e.g. minimizing a specific loss function), or an empirical assessment (e.g. evaluation results conducted before and after the stage).","The most relevant documents are the DeepSeek-V3 and DeepSeek-R1 technical reports. The relevant passages state: ""During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The
pre-training process is remarkably stable. Throughout the entire training process, we did not
encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage
context length extension for DeepSeek-V3. In the first stage, the maximum context length is
extended to 32K, and in the second stage, it is further extended to 128K. Following this, we
conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)
on the base model of DeepSeek-V3, to align it with human preferences and further unlock its
potential"" to build V3. Then, ""To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training
pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the
DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-
Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection
sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains
such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.
After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking
into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to
as DeepSeek-R1""."
Code access,Does the developer release code that allows third-parties to train and run the model?,The released code does not need to match the code used internally. ,"No information provided about code for building DeepSeek-R1, though inference code is released"
Organization chart,How are employees developing and deploying the model organized internally? ,"To receive a point, the developer should provide both the internal organization chart for the team developing the model as well as the headcounts (or a proportion of headcounts) by the team.","The DeepSeek V3 report notes contributors for Research & Engineering, Data Annotation, and Business & Compliance. The DeepSeek R1 report notes core contributors and contributors."
Model cost,What is the cost of building the model?,"Monetary cost should be reported in appropriate currency (e.g. USD), along with the measurement methodology, which may involve estimation. Cost should be reported to a precision of one significant figure (e.g. 200 million USD). ","The most relevant document is the DeepSeek-V3 report with the passage on training costs: ""Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined
with 119K GPU hours for the context length extension and 5K GPU hours for post-training,
DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of
the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M."" For DeepSeek-R1, the developer reports $294,000 USD cost."
Basic model properties,Are all basic model properties disclosed?,"Basic model properties include: the input modality, output modality, model size, model components, and model architecture. To receive a point, all model properties should be disclosed. Modalities refer to the types or formats of information that the model can accept as input. Examples of input modalities include text, image, audio, video, tables, graphs. Model components refer to distinct and identifiable parts of the model. We recognize that different developers may use different terminology for model components, or conceptualize components differently. Examples include: (i) For a text-to-image model, components could refer to a text encoder and an image encoder, which may have been trained separately. (ii) For a retrieval-augmented model, components could refer to a separate retriever module. Model size should be reported in appropriate units, which generally is the number of model parameters, broken down by named component. Model size should be reported to a precision of one significant figure (e.g. 500 billion parameters for text encoder, 20 billion parameters for image encoder). Model architecture is the overall structure and organization of a foundation model, which includes the way in which any disclosed components are integrated and how data moves through the model during training or inference. We recognize that different developers may use different terminology for model architecture, or conceptualize the architecture differently; a sufficient disclosure includes any clear, though potentially incomplete, description of the model architecture.","""We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing.""

The DeepSeek Hugging Face documentation, notes that the number of parameters associated with the model is 671 billion.

""Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models."""
Deeper model properties,Is a detailed description of the model architecture disclosed?,"To receive a point, the model architecture should be described in enough detail to allow for an external entity to fully implement the model. Publicly available code or a configuration file for a model training library (e.g., GPT-NeoX) would be a sufficiently detailed description.",Configuration file released on Hugging Face
Model dependencies,Is the model(s) the model is derived from disclosed?,"We will award this point for a comprehensive disclosure of the model or models on which the foundation model directly depends on or is derived from, as well as the method by which it was derived (e.g., through fine tuning, model merging, or distillation). Additionally, we will award a point if the developer discloses that the model is not dependent on or derived from any model.","""Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning."""
Benchmarked inference,Is the compute and time required for model inference disclosed for a clearly-specified task on clearly-specified hardware?,The duration should be reported in seconds to a precision of one significant figure (e.g. 0.002 seconds). Compute usage for inference should be reported in FLOPs/second to a precision of one significant figure (e.g. 5 x 10^21 FLOPs/second). The hardware in this evaluation need not be the hardware the developer uses for inference. The developer can report this figure over some known or public dataset.,
Researcher credits,Is a protocol for granting external entities API credits for the model disclosed?,"A model credit access protocol refers to the steps, requirements, and considerations involved in granting credits to external entities. We will award this point if the developer discloses key details of its protocol, including (i) where external entities can request access to credits (e.g. via an access request form); (ii) explicit criteria for selecting external entities; and (iii) its policy on granting a transparent decision on whether access has been granted within a specified, reasonable period of time. Additionally, we will award a point if the developer discloses that it does not grant external entities API credits.",
Specialized access,Does the developer disclose if it provides specialized access to the model?,"Specialized access could include several categories, such as early access, subsidized access, or deeper access (e.g., to model weights or checkpoints, that are not publicly available). We will award this point if the developer discloses (i) if it provides specialized access and (ii) statistics on the number of users granted access across academia, industry, non-profits, and governments, to one significant figure.",
Open weights,Are the model's weights openly released?,"To receive this point, model weights need to be publicly available at no cost. Developers may receive this point even if there are some restrictions on the external entities that are permitted access (e.g. geographic restrictions), insofar as these restrictions are transparent (e.g. via a license or some high-level description of who has been granted access to the foundation model).","""ðŸ”„ DeepSeek-R1 is now MIT licensed for clear open access"""
Agent Protocols,Are the agent protocols supported for the model disclosed?,"Agent protocols are specifications that define how autonomous agents exchange messages, context, or function calls with other agents, tools, or services (e.g., Anthropicâ€™s Modelâ€¯Contextâ€¯Protocolâ€¯(MCP) and Googleâ€™s Agentâ€‘toâ€‘Agentâ€¯(A2A) spec). To earn this point, documentation must enumerate each protocol and describe any deviations or proprietary extensions. ","The AI Agent frameworks table in GitHub includes a number of frameworks such as smolagents, YoMo, and superagentx"
Capabilities taxonomy,Are the specific capabilities or tasks that were optimized for during post-training disclosed?,"Capabilities refer to the specific and distinctive functions that the model can perform. We recognize that different developers may use different terminology for capabilities, or conceptualize capabilities differently. We will award this point for a list of capabilities specifically optimized for in the post-training phase of the model, even if some of the capabilities are not reflected in the final model.","""We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeekR1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the modelâ€™s reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models."""
Capabilities evaluation,Does the developer evaluate the model's capabilities prior to its release and disclose them concurrent with release?,"The evaluations must contain precise quantifications of the model's behavior in relation to the capabilities specified in the capabilities taxonomy. We will award this point for any clear, but potentially incomplete, evaluation of multiple capabilities.","""We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1 , LiveCodeBench (Jain et al., 2024) (2024-08 â€“ 2025-01), Codeforces 2 , Chinese National High School Mathematics Olympiad (CNMO 2024)3 , and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench."""
External reproducibility of capabilities evaluation,Are code and prompts that allow for an external reproduction of the evaluation of model capabilities disclosed?,"The released code and prompts need not be the same as what is used internally, but should allow the developer's results on all capability evaluations to be reproduced. The released code must be open source, following the OSI definition of open source.","""We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1 , LiveCodeBench (Jain et al., 2024) (2024-08 â€“ 2025-01), Codeforces 2 , Chinese National High School Mathematics Olympiad (CNMO 2024)3 , and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench."""
Train-test overlap,Does the developer measure and disclose the overlap between the training set and the dataset used to evaluate model capabilities?,"We will award this point if, with every capability evaluation for which the developer reports results, the developer reports the overlap between the training set of the model and the dataset used for evaluation, as well as the general methodology for computing train-test overlap (e.g. a description of how n-gram matching was used). ",
Risks taxonomy,Are the risks considered when developing the model disclosed?,"Risks refer to possible negative consequences or undesirable outcomes that can arise from the model's deployment and usage. These consequences or outcomes may arise from model limitations (functions that the model cannot perform) or issues with the model's trustworthiness (e.g., its lack of robustness, reliability, calibration). We recognize that different developers may use different terminology for risks, or conceptualize risks differently. We will award this point for a complete list of risks considered, even if some of the risks are not reflected in the final model.","https://cdn.deepseek.com/policies/en-US/model-algorithm-disclosure.html
""Risks associated with AI models may arise from two causes:  
1. Limitations due to the immaturity of AI technology.  
2. Risks due to the misuse of AI technology.  
Specifically:""
Under ""Limitations"":
- Hallucination
Under ""Miseuse risks"":
- Privacy protection
- Copyright
- Data security
- Content safety
- Bias
- Discrimination

https://arxiv.org/pdf/2501.12948
2.3.4. Reinforcement Learning for all Scenarios
To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the modelâ€™s helpfulness and harmlessness while simultaneously refining its reasoning capabilities.
...
For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.


https://cdn.deepseek.com/policies/en-US/deepseek-terms-of-use.html
3.4 You will not use the Services to generate, express or promote content or a chatbot that:

(1) is hateful, defamatory, offensive, abusive, tortious or vulgar;

(2) is deliberately designed to provoke or antagonize another or is bullying or trolling another;

(3) may harass, intimidate, threaten, harm, hurt, scare, distress, embarrass or upset another;

(4) is discriminatory such as discriminating another based on race, gender, sexuality, religion, nationality, disability or age;

(5) is pornographic, obscene, or sexually explicit (e.g., sexual chatbots);

(6) facilitates, promotes, incites or glorifies violence or terrorist/extremism content;

(7) exploits, harms, or attempts to exploit or harm or minors or exposes minors to such content; 

(8) are designed to specifically appeal to or present a persona of any person under the age of 18 or the minimum age required in your country;

(9) constitute, encourage or provide instructions for a criminal offence; or

(10) impersonates or is designed to impersonate a celebrity, public figure or a person other than yourself without clearly labelling the content or chatbot as ""unofficial"" or ""parody"", unless you have that person's explicit consent.

4.4 All Outputs provided by this service are generated by an artificial intelligence model and may contain errors or omissions, for your reference only. You should not treat the Outputs as professional advice. Specifically, when using this service to consult on medical, legal, financial, or other professional issues, please be aware that this service does not constitute any advice or commitment and does not represent the opinions of any professional field. If you require related professional services, you should consult professionals and make decisions under their guidance. The Outputs of this software should not be the basis for your further actions or inactions. Any judgment you make based on the Outputs or subsequent related actions you take will result in consequences and responsibilities borne by you alone, including risks arising from reliance on the truthfulness, accuracy, reliability, non-infringement, or suitability for a specific purpose of the Outputs. You should recognize and use generative artificial intelligence technology scientifically and rationally, and in accordance with the law.

4.5 To prevent confusion and misidentification, and to eliminate or reduce the potential impact of inaccurate generated information, DeepSeek has added prominent disclaimer notices at the end of generated text or at the bottom of interactive interfaces, specifically reminding users that the content is AI-generated.

4.6 DeepSeek can provide incomplete, inaccurate, or offending output. If the output refers to a third-party product or service, this does not mean that the third party approves the comment or is affiliated with DeepSeek.


3.6 You will not use the Services for the following improper purposes:

(1) Using the Services for any illegal purposes that violate laws and regulations, regulatory policies, or infringe on the legitimate rights and interests of third parties.

(2) Using the Services for dangerous purposes that may have serious harmful impacts on physical health, psychology, society, or the economy, or violate scientific and technological ethics.

(3) Engaging in activities that infringe on intellectual property rights, trade secrets, and other violations of business ethics, or using algorithms, data, platforms, etc., to implement monopolistic and unfair competition behaviors.

(4) Without DeepSeek's authorization, copying, transferring, leasing, lending, selling, or sub-licensing the entire or part of the Services.

(5) Other uses prohibited or restricted by laws and administrative regulations, or that may harm DeepSeek's interests.

https://chat.deepseek.com/downloads/DeepSeek%20User%20Agreement.pdf
3.4 DeepSeek advises that, as a user of this Service, you shall not input content that induces
outputs violating relevant laws and regulations, including but not limited to:
(1) opposing the fundamental principles established by the Constitution;
(2) endangering national security, leaking state secrets, subverting state power, overthrowing
the socialist system, and undermining national unity;
(3) damaging the honor and interests of the state;
(4) inciting ethnic hatred and ethnic discrimination and undermining national unity;
(5) insulting and abusing the image of martyrdom, denying the deeds of martyrdom, and
beautifying and whitewashing the acts of war of aggression;
(6) undermining the state's religious policy, promoting cults and feudal superstitions;
(7) spreading rumors, false and harmful information, disturbing the social order and
undermining social stability;
(8) promoting obscenity, pornography, gambling, violence, murder, terrorism, extremism or
abetting crime;
(9) inciting unlawful assemblies, associations, processions, demonstrations, or gathering of
people to disrupt social order;
(10) slandering others, revealing their privacy, infringing on their honor, portrait, privacy,
intellectual property rights and other legitimate rights and interests;
(11) undermining international relations as well as international peace and stability;
(12) containing information with other contents prohibited by laws and administrative
regulations.

3.5 DeepSeek advises that, as a user of this Service, you shall not input content that induces
unfriendly dialogue outputs, including but not limited to:
(1) personally attacking and abusing others;
(2) making remarks of a cursing, discriminatory, or indifferent to the dignity of life nature
against the following groups of people: people of different specific nationalities, geographic
regions, genders, gender identities, sexual orientations, races, ethnicities, health conditions,
occupations, ages, beliefs, and handicapped groups;
(3) cursing, intimidating or threatening others;
(4) making direct derogatory comments about content created by other users;
(5) using vulgar terms towards other users and creating offense;
(6) making comments of a prejudicial nature against the following groups of people, groups
that include: specific nationalities, geographic regions, genders, gender identities, sexual
orientations, races, ethnicities, health conditions, occupations, ages, beliefs, handicapped
groups, etc.

3.6 DeepSeek advises that, as a user of this Service, you shall not maliciously contend with the
filtering mechanisms of this Service by, including but not limited to:
(1) inputting characters, numbers and other meaningless garbled codes that are difficult to
recognize the meaning and affect the reading experience;
(2) malicious antagonistic behavior, including but not limited to the use of variants, harmonies
and other ways to circumvent the Service detection to enter contents that violates the above
3.4 and 3.5.
3.7 DeepSeek advises that, as a user of this Service, you shall not interfere with the normal
operation of this Service or harm DeepSeek's legal rights, including but not limited to:
(1) using the Service to engage in illegal and criminal activities such as stealing trade secrets
and personal information;
(2) reversing compiling, reversing engineer or obtaining any information related to the
algorithms of the Service by any other means;
(3) using the Service to develop other products and services that compete with the Service
(unless the restriction violates relevant legal norms);
(4) crawling and copying the Service and any content contained therein by any means
(including but not limited to through any robots, crawler technology other automated settings,
setting up mirrors);
(5) removing or tampering with the artificial intelligence-generated logos or the prominent
logos of the deeply synthesized content involved in the Service without the consent of Depth
Seeker or without a legally justifiable basis for such removal or tampering;
(6) uploading, posting, e-mailing or otherwise transmitting software viruses or other
computer codes, files and programs that interfere with, damage or limit the functionality of
any computer software, hardware or communication equipment;
(7) unlawful intrusion into the servers of the Service, tampering with the code related to the
Service, or other acts that damage the Service;
(8) attempting to circumvent the security settings or network systems of the DeepSeek,
including obtaining data that the user shall not have access to, logging into servers or
accounts that are not explicitly authorized, or employing methods such as running port scans
to snoop on the security measures of other networks;
(9) obtaining output in a manner that causes an unreasonable load to be placed on
DeepSeek's computer systems or facilities, or engaging in behavior that could cause such a
situation to arise;
(10) unlawfully reselling or making available to the public the DeepSeekâ€™s Service or using the
DeepSeekâ€™s Service for commercial purposes without authorization from DeepSeek.
3.8 DeepSeek advises that, as a user of this Service, you shall not engage in behaviors that
violate other legal regulations or infringe upon the legal rights of others or DeepSeek,
such as monopolistic or unfair competition practices."
Risks evaluation,Does the developer evaluate the model's risks prior to its release and disclose them concurrent with release?,The evaluations must contain precise quantifications of the model's behavior in relation to the risks specified in the risk taxonomy. We will award this point for clear evaluations of the majority of the states risks.,
External reproducibility of risks evaluation,Are code and prompts to allow for an external reproduction of the evaluation of model risks disclosed?,"The released code and prompts need not be the same as what is used internally, but should allow the developer's results on all risk evaluations to be reproduced. The released code must be open-source, following the OSI definition of open-source.",
Pre-deployment risk evaluation,Are the external entities have evaluated the model pre-deployment disclosed?,"By external entities, we mean entities that are significantly or fully independent of the developer. We will award this point if the developer specifies the entity that carried out the pre-deployment analysis, discloses the terms of the analysis (such as conditions for releasing the evaluation results or the developer's control over the final results), as well as any financial transaction between the parties. We will award this point if the developer discloses no external entities have evaluated the model pre-deployment, or discloses only terms of the analysis where it is not bound by NDA while still naming all external entities.",
External risk evaluation,Are the parties contracted to evaluated model risks disclosed?,"We will award this point if the developer discloses statistics regarding all contracted parties that are responsible for evaluating risks (not limited to external entities or pre-deployment evaluation). This includes the number of contracted for-profit or non-profit entities, government entities, independent contractors, and researchers contracted by the developer to evaluate risks. We will award this point if the developer discloses it has no such contracts.",
Mitigations taxonomy,Are the post-training mitigations implemented when developing the model disclosed?,"By post-training mitigations, we refer to interventions implemented by the developer during the post-training phase to reduce the likelihood and/or the severity of the modelâ€™s risks. We recognize that different developers may use different terminology for mitigations, or conceptualize mitigations differently. We will award this point for a complete list of mitigations considered, even if some of the mitigations are not reflected in the final model. Alternatively, we will award this point if the developer reports that it does not mitigate risk in this way.","2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the modelâ€™s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. ... For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness."
Mitigations taxonomy mapped to risk taxonomy,Does the developer disclose how the post-training mitigations map onto the taxonomy of risks?,"We will award this point for a complete mapping of the primary risk that each mitigation is meant to address, even if the mitigation potentially maps on to other risks in the taxonomy. Alternatively, we will award this point if the developer reports that it does not mitigate risk.",
Mitigations efficacy,Does the developer evaluate and disclose the impact of post-training mitigations?,"We will award this point if the developer discloses the results on the risk evaluations before and after the post-training mitigations are applied. Alternatively, we will award this point if the developer reports that it does not mitigate risk in this way.",
External reproducibility of mitigations evaluation,Are code and prompts to allow for an external reproduction of the evaluation of post-training mitigations disclosed?,"The released code and prompts need not be the same as what is used internally, but should allow the developer's results on all mitigations evaluations to be reproduced. The released code must be open-source, following the OSI definition of open-source. Alternatively, we will award this point if the developer reports that it does not mitigate risk.",
Model theft prevention measures,Does the developer disclose the security measures used to prevent unauthorized copying (â€œtheftâ€) or unauthorized public release of the model weights?,"This indicator assesses the developer's disclosures regarding how it addresses the risk that malicious actors or insiders could exfiltrate or replicate proprietary weights. Security measures could include insider threat analysis and detection, in addition to external threat management. Examples of such measures include encryption at rest, key management, remote attestation, or auditing for suspicious queries. We will award a point if the developer discloses specific steps taken to safeguard the model weights or that none are implemented.",
Release stages,Are the stages of the model's release disclosed?,"Release stages include A/B testing, release on a user-facing product, GA release, open-weight release, etc.  We recognize that the release of a foundation model falls along a spectrum, with many forms of partial release, and that different developers may conceptualize release differently. We will award a point if the developer provides a clear identification of the stages through which the model was released.","DeepSeek-R1-Lite-Preview released November 2024 as preview version (URL: https://api-docs.deepseek.com/news/news1120)
DeepSeek-R1 full release January 20, 2025 with open-source model and API (URL: https://api-docs.deepseek.com/news/news250120)
DeepSeek-R1-0528 version upgrade released May 28, 2025 (URL: https://api-docs.deepseek.com/news/news250528)
"
Risk thresholds,Are risk thresholds disclosed?,"Risk thresholds determine when a risk level is unacceptably high to a developer (e.g. leading to the decision to not release a model), moderately high (e.g. triggering additional safety screening), or low enough to permit normal usage.

We will award this point if the developer discloses explicit risk thresholds that clarify (i) which harmful outcomes are being scored, (ii) how the scores are computed (in general terms, not necessarily disclosing internal algorithms), and (iii) what triggers an action to block, delay, or otherwise modify a model's release. 

Alternatively, we will award a point if the developer discloses that it does not consider explicit risk thresholds during model release. ",
Versioning protocol,Is there a disclosed protocol for versioning and deprecation of the model?,"We will award a point if the developer discloses how model versions are labeled, updated, deprecated, and communicated to users.","https://api-docs.deepseek.com/
* The deepseek-chat model points to DeepSeek-V3-0324. You can invoke it by specifying model='deepseek-chat'.

* The deepseek-reasoner model points to DeepSeek-R1-0528. You can invoke it by specifying model='deepseek-reasoner'.

https://api-docs.deepseek.com/updates
Version: 2025-05-28
deepseek-reasoner
deepseek-reasoner Model Upgraded to DeepSeek-R1-0528"
Change log,Is there a disclosed change log for the model?,"We will award a point if the developer publishes a version-by-version record of new features, fixes, or performance improvements.",Version: 2025-05-28 deepseek-reasoner deepseek-reasoner Model Upgraded to DeepSeek-R1-0528: Enhanced Reasoning Capabilities Significant benchmark improvements (Pass@1) AIME 2025: 70.0 â†’ 87.5 (+17.5) GPQA: 71.5 â†’ 81.0 (+9.5) LCB_v6: 63.5 â†’ 73.3 (+9.8) Aider: 57.0 â†’ 71.6 (+14.6) Note: Complex reasoning tasks may consume more tokens compared to legacy R1 version. Optimized Front-end Development Generated web pages and games now feature improved aesthetics. Reduced Hallucinations Significantly suppressed hallucination issues present in legacy R1 version. JSON Output & Function Calling Support Function call performance: Tau-bench score: 53.5 (Airline) / 63.9 (Retail)
Foundation model roadmap,"Is a forward-looking roadmap for upcoming models, features, or products disclosed?","A foundation model roadmap is a transparent statement about how the developer intends to evolve or expand its LLM offerings, including upcoming models, major feature releases, or expanded products based on the model, along with approximate timelines or version milestones. It can be high-level (e.g., â€œnew model Q2 2025â€), but must exist publicly.","""This is just the beginning! Look forward to multimodal support and other cutting-edge features in the DeepSeek ecosystem."""
Top distribution channels,Are the top-5 distribution channels for the model disclosed?,"We define distribution channels to be either an API provider (a pathway by which users can query the model with inputs and receive outputs) or a model distributor (a pathway by which model weights are released). We recognize that distribution channels may arise without the knowledge of the model developer. For example, the weights of a model may be released through one distribution channel and then be distributed through other channels.

Distribution channels can be ranked by any reasonable metric (e.g., number of queries, number of downloads, number of users, revenue). A description of the metric should be provided.

API providers and model distributors may be ranked separately using different metrics as long as the total number of distribution channels equals five (if five distribution channels exist). For example, the developer may choose to disclose the top-3 API providers (ranked by the number of queries) and the top-2 model distributors (ranked by the number of downloads).","DeepSeek API: https://api-docs.deepseek.com/guides/reasoning_model
ModelScope: https://huggingface.co/deepseek-ai/DeepSeek-R1-0528
HuggingFace: https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528/summary"
Quantization,Is the quantization of the model served to customers in the top-5 distribution channels disclosed?,We will award this point for a disclosure of the model precision in each of the top-5 distribution channels. ,"Model weights quantization example config:
```
  ""quantization_config"": {
    ""activation_scheme"": ""dynamic"",
    ""fmt"": ""e4m3"",
    ""quant_method"": ""fp8"",
    ""weight_block_size"": [
      128,
      128
    ]
  }
```

API quantization:
""All DeepSeek-V3/R1 inference services are served on H800 GPUs with precision consistent with training. Specifically, matrix multiplications and dispatch transmissions adopt the FP8 format aligned with training, while core MLA computations and combine transmissions use the BF16 format, ensuring optimal service performance."""
Terms of use,Are the terms of use of the model disclosed?,"We define terms of use to include terms of service and model licenses. We will award this point for a pointer to the terms of service or model license.

In the event that model's licenses are written more generally, it should be clear which assets they apply to. We recognize that different developers may adopt different business models and therefore have different types of model licenses. Examples of model licenses include responsible AI licenses, open-source licenses, and licenses that allow for commercial use.

Terms of service should be disclosed for each of the top-5 distribution channels. However, we will award this point if there are terms-of-service that appear to apply to the bulk of the modelâ€™s distribution channels.","This code repository and the model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:


https://cdn.deepseek.com/policies/en-US/deepseek-terms-of-use.html"
Distribution channels with usage data,What are the top-5 distribution channels for which the developer has usage data?,"We define distribution channels to be either an API provider (a pathway by which users can query the model with inputs and receive outputs) or a model distributor (a pathway by which model weights are released). We recognize that distribution channels may arise without the knowledge of the model developer. For example, the weights of a model may be released through one distribution channel and then be distributed through other channels. 

Distribution channels can be ranked by any reasonable metric (e.g., number of queries, number of downloads, number of users, revenue). A description of the metric should be provided.


We define usage data as any form of developer-exclusive data collected from any of a developer's distribution channel. A developer has access to usage data from a distribution channel if it is able to use that data for downstream purposes (e.g., analytics, training etc.). Usage data may be shared outside of the developer, but it is initially collected by the distribution channel and shared to the developer.",This DeepSeek website implies they collect user data for their own services (deepseek chat and API)
Amount of usage,"For each of the top-5 distribution channels, how much usage is there?","Usage should be reported as the number of queries over the span of a month, reported to the precision of one significant figure (e.g., 50 million queries).",HuggingFace provides the number of model downloads
Classification of usage data,"Is a representative, anonymized dataset classifying queries into usage categories disclosed?","Developers may either share a fully public dataset or a partially restricted dataset (e.g., under a research license). We will award this point if there is a clear, aggregated or sample dataset that reveals categories of tasks/queries.",No information provided about usage data.
Data retention and deletion policy,Is a policy for data retention and deletion disclosed?,"A data retention and deletion policy is a policy for removing particular data from the training set and/or preventing it from being used if there is a user or external request (e.g., â€œright to be forgottenâ€) that also covers internal data governance. This includes whether there is a formal process to delete or retract data from future training runs and how long raw data is retained. It also clarifies how quickly deletions propagate to the model (e.g., â€œonly in subsequent major model releasesâ€).","When the Personal Data collected is no longer required by us, we and our service providers will perform the necessary procedures for destroying, deleting, erasing, or converting it into an anonymous form as permitted or required under applicable laws.

While DeepSeek clarifies that usage data isn't used for pre-training, it might be used for optimization: https://cdn.deepseek.com/policies/en-US/model-algorithm-disclosure.html"
Geographic statistics,"Across all forms of downstream use, are statistics of model usage across geographies disclosed?","We will award this point if there is a meaningful, though potentially incomplete or vague, disclosure of geographic usage statistics at the country-level.",No information provided about usage data.
Internal products and services,What are the top-5 internal products or services using the model?,"An internal product or service is a product or service built by the developer. Products or services can be ranked by any reasonable metric (e.g., number of users, queries, revenue). A description of the metric should be provided.","DeepSeek App, DeepSeek Chat, DeepSeek Platform, API"
External products and services,What are the top-5 external products or services using the model?,"An external product or service is a product or service built by a party external to the developer. Products or services can be ranked by any reasonable metric (e.g., number of users, queries, revenue). A description of the metric should be provided.

We will award a point if the developer discloses that that it does not have access to such metrics about external products or services.","Multiple external integrations listed in awesome-deepseek-integration repository, including TigerGPT, HIX.AI, PopAi, and 1AI iOS Chatbot"
Users of internal products and services,How many monthly active users are there for each of the top-5 internal products or services using the model?,"An internal product or service is a product or service built by the developer. The number of users refers to users who engaged or interacted with the model through the internal product or service over the last month or averaged over the last X months (this should be specified). Number of users should be specified to one significant figure (e.g. 100,000).",No information provided about users of internal products and services.
Consumer/enterprise usage,"Across all distribution channels for which the developer has usage data, what portion of usage is consumer versus enterprise?","Consumer usage refers to usage by individual consumers. Enterprise usage refers to usage by enterprise customers (including government use). Consumer and enterprise usage should be calculated in terms of the number of queries by or the amount of revenue from consumer or enterprise users. Percentages should be specified to two significant digits (e.g., 12% consumer, 88% enterprise).",No information provided about consumer versus enterprise usage.
Enterprise users,"Across all distribution channels for which the developer has usage data, what are the top-5 enterprises that use the model?",Enterprises should be ranked by the number of queries made or the amount of revenue from usage since the model's release. We will also award this point if the developer indicates it does not have access to enterprise usage data.,No information provided about enterprise users.
Government use,What are the 5 largest government contracts for use of the model?,"This includes known government contracts of enterprise or government-specific products and services that use the model. We will award this point if the developer discloses its top five government contracts ranked monetary value, though the developer may omit contracts where it is under NDA regarding the existence of the contract.",No information provided about government users.
Benefits Assessment,Is an assessment of the benefits of deploying the model disclosed?,We will award this point for any quantitative assessment of the benefits or potential benefits of deploying the model.,No information provided about benefits assessments.
AI bug bounty,Does the developer operate a public bug bounty or vulnerability reward program under which the model is in scope? ,"We will award this point for a publicly documented bug bounty or vulnerability reward program describing (i) in-scope vulnerabilities (e.g., prompt bypasses, data leaks), (ii) out-of-scope items, (iii) submission process, and (iv) reward tiers or recognition if applicable. We will award a point if the developer discloses it has no AI bug bounty that encourages external researchers to report security, privacy, or adversarial vulnerabilities in the model.",No information provided about an AI bug bounty.
Responsible disclosure policy,Does the developer clearly define a process by which external parties can disclose model vulnerabilities or flaws? ,"We will award this point for a description of the process external parties can use for responsbly disclosing model vulnerabilities and flaws, which should include (i) what mechanism external parties can use to disclose vulnerabilities or flaws (e.g., a form, an email) and (ii) what process follows a disclosure (e.g., how much time must parties wait until public release). This is often included with a bug bounty, but can also be standalone. We will award a point if the developer discloses it has no responsible disclosure policy.",No information provided about responsible disclosure.
Safe harbor,Does the developer disclose its policy for legal action against external evaluators conducting good-faith research? ,"We will award this point if the developer discloses whether it has a policy committing it to not pursue legal action against external evaluators conducting good-faith research. This should not be only for software security vulnerabilities, but also AI flaws, and it should be based on researcher conduct standards, not at the sole discretion of the company. We will award this point if the developer provides a clear description of its policy regarding such protections for external researchers, or lack thereof. ","3.5 You will not violate any applicable, nor interfere with, damage, or attack the Services, systems, networks, models, and other components that support the normal operation of the service. You will not engage in activities that endanger the security of the Services: (1) Engage in illegal activities involving network intrusion, such as: using unauthorized data or accessing unauthorized servers/accounts; forging TCP/IP packet names or partial names; attempting to probe, scan, or test vulnerabilities in the software system or network without permission."
Security incident reporting protocol,Are major security incidents involving the model disclosed? ,"A security incident reporting protocol provides post-deployment transparency about serious incidents or breaches. Security incidents refer to incidents where external security threats affect the model (e.g., data breaches or DDoS attacks on the service). We will award this point if the developer states (i) how to submit a security incident report, (ii) how quickly it will respond, and (iii) when and whether results are disclosed.

Every incident need not be reported publicly, but the developer must disclose a policy determining how incidents are reported and disclosed.","The ""report a vulnerability"" button at the bottom of the page opens a website to security@deepseek.com"
Misuse incident reporting protocol,Are misuse incidents involving the model disclosed? ,"A misuse incident reporting protocol provides post-deployment transparency about incidents of misuse involving the model. As opposed to the previous indicator, this indicator is about actors misusing the model to cause real-world harm, such as misinformation operations or cybersecurity attacks. We will award this point if the developer states (i) how to submit a misuse incident report, (ii) how quickly it will respond, and (iii) when and whether results are disclosed.

Every incident need not be reported publicly, but there needs to be a policy governing how incidents are reported.","The ""report a vulnerability"" button at the bottom of the page opens a website to security@deepseek.com"
Post-deployment coordination with government,Does the developer coordinate evaluation with government bodies?,"We will award this point if the developer specifies which government bodies it is coordinating with and for what types of post-deployment evaluations. Government bodies include AI Safety Institutes, national security agencies, national labs, and international governmental enties such as UN agencies or the G7. Evaluation here may also include sharing of the developer's proprietary evaluation results for help with interpretation. ",No information provided on coordination of post-deployment evaluation with government bodies.
Feedback mechanisms,"Does the developer disclose a way to submit user feedback? If so, is a summary of major categories of feedback disclosed?","We will award this point if the developer (i) discloses how users can submit feedback (e.g., via a form or a thumbs up/thumbs down for model responses) and (ii) discloses aggregated or categorized feedback data (e.g. a categorization of thumbs up and thumbs down data). ","Users can evaluate model output through liking or disliking actions. The open platform TOS states:

10.2 If you notice any violation of laws and regulations or breach of these Terms or you have any opinions or suggestions regarding these Terms or the Service, you can contact us through the following methods or directly contact our sales staff: 
Online Complaints and Feedback Portal: Click the ""Contact us"" button on the product interface after logging in. 
Contact Email: api-service@deepseek.com
Contact Address: 5th Floor, North Building, Block C, Rongke Information Center, No.2 South Science Academy Road, Haidian District, Beijing, China.
"
"Permitted, restricted, and prohibited model behaviors","Are model behaviors that are permitted, restricted, and prohibited disclosed?","We refer to a policy that includes this information as a model behavior policy, or a developer's policy on what the foundation model can and cannot do (e.g. such a policy may prohibit a model from responding to NSFW content). We recognize that different developers may adopt different business models and that some business models may make enforcement of a model behavior policy more or less feasible. We will award this point if at least two of the three categories (i.e. permitted, restricted, and prohibited model behaviors) are disclosed. Alternatively, we will award this point if the developer reports that it does not impose any restrictions on its model's behavior in this way.","To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the modelâ€™s helpfulness and harmlessness while simultaneously refining its reasoning capabilities
"
Model response characteristics,Are desired model response characteristics disclosed?,"Model response characteristics include default behaviors or behaviors that the developer steers the model to take. These may include being helpful, taking an objective point of view, or using tools only when necessary. We will award points for a clear description of desired model response characteristics or a statement that there are no such characteristics.","For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness."
System prompt,Is the default system prompt for at least one distribution channel disclosed?,A system prompt is defined as the prompt provided to the system by default that guides the system's behavior. We will award this point for the disclosure of the verbatim text of the full system prompt as well as an explanation for the context in which the system prompt is used.,"""          {""role"": ""system"", ""content"": ""You are a helpful assistant.""},"""
Intermediate tokens,Are intermediate tokens used to generate model outputs available to end users? ,"Intermediate tokens are defined as any tokens generated by the model before the final output is shown to the user, such as model chains of thought. We will also award this point if a summary of intermediate tokens is made available to end users. If intermediate tokens or summaries are not made available, the developer should provide a justification.",R1's output includes reasoning content
Internal product and service mitigations,"For internal products or services using the model, are downstream mitigations against adversarial attacks disclosed?","An internal product or service is a product or service built by the developer. Adversarial attacks include prompt injection, jailbreaking, or malicious queries. Mitigations against adversarial attacks might include specialized prompt filtering, content scanning, or real-time monitoring of queries or accounts. We will award this point if the developer discloses a clear statement of methods used (e.g., a specialized prompt sanitizer or adversarial pattern detector), or if the developer states it does not implement such product-level mitigations against adversarial attacks.",Safety monitoring services mentioned in privacy policy
External developer mitigations,Does the developer provide built-in or recommended mitigations against adversarial attacks for downstream developers?,"Downstream developers are developers who access the model through a distribution channel. Adversarial attacks include prompt injection, jailbreaking, or malicious queries. Mitigations against adversarial attacks that developers might build in or recommend include content filtering endpoints and recommended prompt templates. We will award this point if the developer discloses (i) technical mitigations (e.g., a developer provided moderation API or classifier) it offers or implements, (ii) recommended best practices or libraries for downstream developers, or (iii) an explicit statement that it does not build or recommend any particular downstream mitigations in this way.. ",No information provided on external developer mitigations.
Enterprise mitigations,Does the developer disclose additional or specialized mitigations for enterprise users?,"Enterprise users are, for example, large organizations with dedicated service agreements or users of enterprise-specific API deployments or products and services. Additional or specialized mitigations may address enterprise needs such as data privacy controls, advanced prompt/response monitoring, or compliance checks with regulations such as GDPR or HIPAA. Additional or specialized mitigations may include single-tenant deployments, custom filters for specific regulated industries, or advanced logging for compliance. We will award a point if the developer at least describes these mitigations or states that it does not provide such additional or specialized enterprise mitigations.",No information provided on specialized mitigations for enterprise users.
Detection of machine-generated content,Are mechanisms that are used for detecting content generated by this model disclosed?,"A mechanism for detecting machine-generated content might include storing a copy of all outputs generated by the model to compare against, implementing a watermark on model outputs, adding cryptographic metadata (such as C2PA), or training a detector post-hoc to identify such content. We will award this point if any such mechanism is disclosed or if the developer reports that it does not have or use any such mechanism.",Terms of Use mention AI-generated content disclosure requirements
Documentation for responsible use,Does the developer provide documentation for responsible use by downstream developers?,"To receive a point, the developer should provide documentation for responsible use. This might include details on how to adjust API settings to promote responsible use, descriptions of how to implement mitigations, or guidelines for responsible use. We will also award this point if the developer states that it does not provide any such documentation. For example, the developer might state that the model is offered as is and downstream developers are accountable for using the model responsibly.",HuggingFace provides usage recommendations
Permitted and prohibited users,Is a description of who can and cannot use the model on the top-5 distribution channels disclosed?,We will award this point for a description of the company's policies for permitted and prohibitted users on its top-5 distribution channels. We will award this point if the developer has a more general acceptable use policy that it confirms applies across these distribution channels. We will award this point if there are no restrictions on users.,"You represent and warrant that Services may not be used in or for the benefit of, or exported, re-exported, or transferred (a) to or within any country subject to comprehensive sanctions under Export Control and Sanctions Laws; (b) to any party on any restricted party lists under any applicable Export Control and Sanctions Laws that would prohibit your use of Services. "
"Permitted, restricted, and prohibited uses","Which uses are explicitly allowed, conditionally permitted, or strictly disallowed under the acceptable use policy for the top-5 distribution channels?","We will award this point for a rough characterization of two or more of permitted, restricted, and prohibited uses across the top-5 distribution channels. We will award this point if the developer has a more general acceptable use policy that it confirms applies across these distribution channels. We will award this point if there are no restrictions on users.","R1 released on HF under an MIT license with no prohibited uses. Section 3.4 of terms of use: """"3.4 You will not use the Services to generate, express or promote content or a chatbot that:..."""""
AUP enforcement process,What are the methods used by the developer to enforce the acceptable policy?,"We will award this point if the developer discloses the processes (automated or manual) it uses to detect, review, and respond to potential acceptable use policy violations. We will award this point for a reasonable best-effort attempt to provide the bulk of this information, though one line indicating the developer reserves the right to terminate accounts is insufficient. Alternatively, we will award this point if the developer reports that it does not use such methods to enforce its acceptable use policy.","For users who violate regulations, DeepSeek has the right, according to its reasonable
judgment and without notice, to take actions such as warnings, functionality restrictions,
service suspensions or terminations, account bans, content deletions, and to save related
records and report to competent authorities"
AUP enforcement frequency,Are statistics on the developer's AUP enforcement disclosed?,"We will award this point if the developer discloses enforcement statistics (e.g., violation counts or actions taken) from its enforcement of its acceptable use policy. Alternatively, we will award this point if the developer reports that it does not enforce its acceptable use policy.",Open Platform Terms describe enforcement actions and record retention for violation
Regional policy variations,Are differences in the developer's acceptable use or model behavior policy across geographic regions disclosed?,"We will award this point if the developer discloses distinctions in its AUP or MBP and provides examples of differences in multiple specific regions, or states that no differences exist. For example, some jurisdictions impose content restrictions beyond those in the developerâ€™s global policy that may necessesitate local deviations.","Terms of Use governed by People's Republic of China laws

Privacy terms have jurisdiction specific sections"
Oversight mechanism,Does the developer have an internal or external body that reviews core issues regarding the model prior to deployment?,"We will award this point if the developer discloses that is has such an internal or external body and provides some description of its scope, or alternatively if the developer discloses that it has no such body. An oversight mechanism covers governance structure beyond mere external risk evaluation, asking whether a formal body regularly reviews design and deployment decisions. Core issues may include model objectives, data usage, or risk mitigation.",No information provided on oversight mechanisms.
Whistleblower protection,Does the developer disclose a whistleblower protection policy?,"We will award this point if the developer discloses (i) the existence of a whistleblower protection policy, (ii) what protections are afforded to whistleblowers, (iii) how reports are handled and investigated, and (iv) any external oversight of the whistleblower protection process. This might include protections for whistleblowers who report safety, ethical, or legal concerns related to the model. We will also award this point if the developer discloses that it has no such policy.",No information provided on whistleblower protection policies.
Government commitments,What commitments has the developer made to government bodies?,We will award this point if the company provides an exhaustive list of commitments it has made to government bodies in the jurisdictions where it offers its model.,"We also participated in the China AI Safety and Security Commitments Framework. Moreover, Chinaâ€™s AI regulatory framework differs significantly from that of the U.S., relying on formal legislation and regulatory oversight rather than solely on voluntary corporate commitments. As such, participation in commitments should not be used as a direct basis for comparison."
